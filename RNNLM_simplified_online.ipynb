{"cells":[{"cell_type":"markdown","metadata":{"id":"hiVGmemCBU5t"},"source":["# Recurrent Neural Network with online BPTT"]},{"cell_type":"markdown","source":["This notebook is based on [DennyBritz's RNN Tutorial](https://github.com/dennybritz/nn-from-scratch). \n","\n","The main change is that training is done in an online way, which means weights are updated after every time steps rather than only once at the end of a sequence. In other words, Forward and Backward have fine grain coupling, running alternatively."],"metadata":{"id":"lmIx1W4xBvAT"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDbE89AZ0KgO","executionInfo":{"status":"ok","timestamp":1682606588645,"user_tz":-120,"elapsed":3270,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"502e866b-9540-495b-fe5c-1a4164e53c33"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","%cd /content/drive/MyDrive/graduation_project/FPTT-on-ANN/RNN/rnn-tutorial-rnnlm-py3/\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PyA3ZOJKqN1","executionInfo":{"status":"ok","timestamp":1682606588646,"user_tz":-120,"elapsed":32,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"0df371fa-e3f4-4bac-8c7b-db277eb6c5ec"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/graduation_project/FPTT-on-ANN/RNN/rnn-tutorial-rnnlm-py3\n","/content/drive/MyDrive/graduation_project/FPTT-on-ANN/RNN/rnn-tutorial-rnnlm-py3\n"]}]},{"cell_type":"code","execution_count":49,"metadata":{"id":"M60m8-QPBU5w","executionInfo":{"status":"ok","timestamp":1682606588646,"user_tz":-120,"elapsed":24,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["import csv\n","import itertools\n","import operator\n","import numpy as np\n","import nltk\n","import sys\n","from datetime import datetime\n","from utils import *\n","#!pip3 install matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"1Bl1Xu5NBU5y","outputId":"388a04a9-ea4b-455d-cab7-470b1b191c14","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682606588862,"user_tz":-120,"elapsed":239,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'book'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Package abc is already up-to-date!\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Package brown is already up-to-date!\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Package chat80 is already up-to-date!\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Package conll2000 is already up-to-date!\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Package conll2002 is already up-to-date!\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package dependency_treebank is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Package ieer is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Package nps_chat is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Package ppattach is already up-to-date!\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    |   Package reuters is already up-to-date!\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Package senseval is already up-to-date!\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Package state_union is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Package swadesh is already up-to-date!\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Package timit is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Package toolbox is already up-to-date!\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Package udhr is already up-to-date!\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Package udhr2 is already up-to-date!\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package unicode_samples is already up-to-date!\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Package webtext is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_tagset is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package book_grammars is already up-to-date!\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package city_database is already up-to-date!\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Package tagsets is already up-to-date!\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection book\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":50}],"source":["# Download NLTK model data (you need to do this once)\n","nltk.download(\"book\")"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"HrwDQ5zkBU51","outputId":"fac577cb-ec5a-49db-c5e5-012b8e9c0189","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682606615542,"user_tz":-120,"elapsed":26683,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading CSV file...\n","Parsed 79184 sentences.\n","Found 63023 unique words tokens.\n","Using vocabulary size 8000.\n","The least frequent word in our vocabulary is 'appointments' and appeared 10 times.\n","\n","Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n","\n","Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n"]}],"source":["vocabulary_size = 8000\n","unknown_token = \"UNKNOWN_TOKEN\"\n","sentence_start_token = \"SENTENCE_START\"\n","sentence_end_token = \"SENTENCE_END\"\n","\n","# Read the data and append SENTENCE_START and SENTENCE_END tokens\n","print(\"Reading CSV file...\")\n","with open('data/reddit-comments-2015-08.csv', 'rt', encoding='utf8') as f:\n","    reader = csv.reader(f, skipinitialspace=True)\n","    next(reader)\n","    # Split full comments into sentences\n","    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n","    # Append SENTENCE_START and SENTENCE_END\n","    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n","print(\"Parsed %d sentences.\" % (len(sentences)))\n","    \n","# Tokenize the sentences into words\n","tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n","\n","# Count the word frequencies\n","word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n","print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n","\n","# Get the most common words and build index_to_word and word_to_index vectors\n","vocab = word_freq.most_common(vocabulary_size-1)\n","index_to_word = [x[0] for x in vocab]\n","index_to_word.append(unknown_token)\n","word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n","\n","print(\"Using vocabulary size %d.\" % vocabulary_size)\n","print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n","\n","# Replace all words not in our vocabulary with the unknown token\n","for i, sent in enumerate(tokenized_sentences):\n","    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n","\n","print(\"\\nExample sentence: '%s'\" % sentences[0])\n","print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"I-SVNTf8BU51","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682606616494,"user_tz":-120,"elapsed":967,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"9ae9c3f0-191c-4ab2-c64c-ce3dedbddc17"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-52-4547043b0b3a>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n","<ipython-input-52-4547043b0b3a>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n"]}],"source":["# Create the training data\n","X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n","y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"]},{"cell_type":"markdown","metadata":{"id":"uO8XUlnQBU52"},"source":["Here's an actual training example from our text:"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"j2GRouHoBU52","outputId":"b2e14f42-80d4-497c-8892-c59546b75fb3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682606616494,"user_tz":-120,"elapsed":11,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["x:\n","SENTENCE_START what are n't you understanding about this ? !\n","[0, 52, 28, 17, 10, 858, 55, 26, 35, 70]\n","\n","y:\n","what are n't you understanding about this ? ! SENTENCE_END\n","[52, 28, 17, 10, 858, 55, 26, 35, 70, 1]\n"]}],"source":["# Print an training data example\n","x_example, y_example = X_train[17], y_train[17]\n","print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n","print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"FoQ71Kb5BU53","executionInfo":{"status":"ok","timestamp":1682606616495,"user_tz":-120,"elapsed":7,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["class RNNNumpy:\n","    \n","    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n","        # Assign instance variables\n","        self.word_dim = word_dim\n","        self.hidden_dim = hidden_dim\n","        self.bptt_truncate = bptt_truncate\n","        # Randomly initialize the network parameters\n","        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n","        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n","        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n","        "]},{"cell_type":"markdown","metadata":{"id":"6n7SHacvBU53"},"source":["#### Forward Propagation\n","\n","Next, let's implement the forward propagation (predicting word probabilities) defined by our equations above:"]},{"cell_type":"code","source":["def softmax(z): return np.exp(z)/((np.exp(z)).sum())"],"metadata":{"id":"d7I9jOSk_pTO","executionInfo":{"status":"ok","timestamp":1682606616495,"user_tz":-120,"elapsed":6,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","execution_count":56,"metadata":{"id":"TKfs1md-BU53","executionInfo":{"status":"ok","timestamp":1682606616495,"user_tz":-120,"elapsed":6,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["def forward_propagation(self, x):\n","    # The total number of time steps\n","    T = len(x)\n","    # During forward propagation we save all hidden states in s because need them later.\n","    # We add one additional element for the initial hidden, which we set to 0\n","    s = np.zeros((T + 1, self.hidden_dim))\n","    s[-1] = np.zeros(self.hidden_dim)\n","    # The outputs at each time step. Again, we save them for later.\n","    o = np.zeros((T, self.word_dim))\n","    # For each time step...\n","    for t in np.arange(T):\n","        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n","        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n","        o[t] = softmax(self.V.dot(s[t]))\n","    return [o, s]\n","\n","RNNNumpy.forward_propagation = forward_propagation"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"wVfEyQBsBU58","executionInfo":{"status":"ok","timestamp":1682606616495,"user_tz":-120,"elapsed":6,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["def predict(self, x):\n","    # Perform forward propagation and return index of the highest score\n","    o, s = self.forward_propagation(x)\n","    return np.argmax(o, axis=1)\n","\n","RNNNumpy.predict = predict"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"5vCCHRSDBU58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682606616496,"user_tz":-120,"elapsed":7,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"7b16fa8c-2b71-4104-8ffd-31e2b3a3a1c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["(45, 8000)\n","[[0.00012408 0.0001244  0.00012603 ... 0.00012515 0.00012488 0.00012508]\n"," [0.00012566 0.00012567 0.0001254  ... 0.00012563 0.00012532 0.00012528]\n"," [0.00012581 0.00012334 0.00012526 ... 0.0001256  0.00012492 0.00012513]\n"," ...\n"," [0.00012441 0.00012512 0.0001248  ... 0.00012496 0.00012448 0.000126  ]\n"," [0.00012493 0.00012393 0.00012497 ... 0.00012428 0.00012527 0.00012465]\n"," [0.00012493 0.00012557 0.00012502 ... 0.00012481 0.00012429 0.00012561]]\n"]}],"source":["np.random.seed(10)\n","model = RNNNumpy(vocabulary_size)\n","o, s = model.forward_propagation(X_train[10])\n","print(o.shape)\n","print(o)"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"kUhQN5ONBU58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682606617421,"user_tz":-120,"elapsed":930,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"345bda20-ddad-4bad-bc94-cc231d72acaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["(45,)\n","[1284  397 2044 5314 3865 1042 7598 6200 1041 1042 7598 3106 6892 4941\n"," 4480 5370 5638 4591 5407 2314 2798 2887  903 4719 7051 5151   18 4223\n"," 6127 1499 1207 1814 7522 4911 4545   35 3528 2314  794 1293 1305 1692\n","  828 2874 7766]\n"]}],"source":["predictions = model.predict(X_train[10])\n","print(predictions.shape)\n","print(predictions)"]},{"cell_type":"markdown","metadata":{"id":"rjqXNJaiBU59"},"source":["#### Calculating the Loss\n","\n","To train our network we need a way to measure the errors it makes. We call this the loss function $L$, and our goal is find the parameters $U,V$ and $W$ that minimize the loss function for our training data. A common choice for the loss function is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression). If we have $N$ training examples (words in our text) and $C$ classes (the size of our vocabulary) then the loss with respect to our predictions $o$ and the true labels $y$ is given by:\n","\n","$\n","\\begin{aligned}\n","L(y,o) = - \\frac{1}{N} \\sum_{n \\in N} y_{n} \\log o_{n}\n","\\end{aligned}\n","$\n","\n","The formula looks a bit complicated, but all it really does is sum over our training examples and add to the loss based on how off our prediction are. The further away $y$ (the correct words) and $o$ (our predictions), the greater the loss will be. We implement the function `calculate_loss`:"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"QLBJxGNZBU59","executionInfo":{"status":"ok","timestamp":1682606617422,"user_tz":-120,"elapsed":11,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["def calculate_total_loss(self, x, y):\n","    L = 0\n","    # For each sentence...\n","    for i in np.arange(len(y)):\n","        o, s = self.forward_propagation(x[i])\n","        # We only care about our prediction of the \"correct\" words\n","        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n","        # Add to the loss based on how off we were\n","        L += -1 * np.sum(np.log(correct_word_predictions))\n","    return L\n","\n","def calculate_loss(self, x, y):\n","    # Divide the total loss by the number of training examples\n","    N = np.sum((len(y_i) for y_i in y))\n","    return self.calculate_total_loss(x,y)/N\n","\n","RNNNumpy.calculate_total_loss = calculate_total_loss\n","RNNNumpy.calculate_loss = calculate_loss"]},{"cell_type":"markdown","metadata":{"id":"mBZU1aLjBU59"},"source":["Let's take a step back and think about what the loss should be for random predictions. That will give us a baseline and make sure our implementation is correct. We have $C$ words in our vocabulary, so each word should be (on average) predicted with probability $1/C$, which would yield a loss of $L = -\\frac{1}{N} N \\log\\frac{1}{C} = \\log C$:"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"bZxT-V2nBU59","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682606634005,"user_tz":-120,"elapsed":16592,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"cc559188-a95c-4866-f984-b911021dccf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Expected Loss for random predictions: 8.987197\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-60-3f0be9da26f3>:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","  N = np.sum((len(y_i) for y_i in y))\n"]},{"output_type":"stream","name":"stdout","text":["Actual loss: 8.987374\n"]}],"source":["# Limit to 1000 examples to save time\n","print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n","print(\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"]},{"cell_type":"markdown","metadata":{"id":"nzwcrSetBU59"},"source":["Pretty close! Keep in mind that evaluating the loss on the full dataset is an expensive operation and can take hours if you have a lot of data!"]},{"cell_type":"markdown","metadata":{"id":"jY9yOFOjBU59"},"source":["#### Training the RNN with SGD and Backpropagation Through Time (BPTT)\n"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"sul9neNqBU5-","executionInfo":{"status":"ok","timestamp":1682606634005,"user_tz":-120,"elapsed":18,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["def bptt(self, x, y):\n","    T = len(y)\n","    # Perform forward propagation\n","    o, s = self.forward_propagation(x)\n","    # We accumulate the gradients in these variables\n","    dLdU = np.zeros(self.U.shape)\n","    dLdV = np.zeros(self.V.shape)\n","    dLdW = np.zeros(self.W.shape)\n","    delta_o = o\n","    delta_o[np.arange(len(y)), y] -= 1.\n","    # For each output backwards...\n","    for t in np.arange(T): #[::-1]:\n","        dLdV += np.outer(delta_o[t], s[t].T)\n","        # Initial delta calculation\n","        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n","        # Backpropagation through time (for at most self.bptt_truncate steps)\n","        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n","            # print(\"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step))\n","            dLdW += np.outer(delta_t, s[bptt_step-1])              \n","            dLdU[:,x[bptt_step]] += delta_t\n","            # Update delta for next step\n","            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n","    return [dLdU, dLdV, dLdW]\n","\n","RNNNumpy.bptt = bptt"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"e-V4xRiCI-EH","executionInfo":{"status":"ok","timestamp":1682606634006,"user_tz":-120,"elapsed":18,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["# online BPTT that updates weights after each time step\n","# to be specific, for each time step\n","# 1. forward\n","# 2. backward\n","# 3. update\n","# bptt_truncate being 0 -- no look back\n","\n","def bptt_online_learning(self, x, y, learning_rate):\n","    T = len(y)\n","\n","    o = np.zeros((T, self.word_dim))\n","    s = np.zeros((T + 1, self.hidden_dim))\n","    s[-1] = np.zeros(self.hidden_dim)\n","    delta_o = o\n","    \n","    for t in np.arange(T):\n","\n","        # Perform forward propagation for one time step\n","        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n","        o[t] = softmax(self.V.dot(s[t]))\n","\n","        # We accumulate the gradients in these variables\n","        dLdU = np.zeros(self.U.shape)\n","        dLdV = np.zeros(self.V.shape)\n","        dLdW = np.zeros(self.W.shape)\n","\n","        delta_o[t, y[t]] -= 1.\n","        \n","        dLdV += np.outer(delta_o[t], s[t].T)\n","        # Initial delta calculation\n","        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n","\n","        # Backpropagation through time (for at most self.bptt_truncate steps)\n","        # for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n","        for bptt_step in np.arange(max(0, t), t+1)[::-1]:\n","            # print(\"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step))\n","            dLdW += np.outer(delta_t, s[bptt_step-1])              \n","            dLdU[:,x[bptt_step]] += delta_t\n","            # Update delta for next step\n","            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n","        \n","        self.U -= learning_rate * dLdU\n","        self.V -= learning_rate * dLdV\n","        self.W -= learning_rate * dLdW \n","\n","RNNNumpy.bptt_online_learning = bptt_online_learning"]},{"cell_type":"markdown","metadata":{"id":"AqYr7EaCBU5-"},"source":["#### SGD Implementation\n","\n","Now that we are able to calculate the gradients for our parameters we can implement SGD. I like to do this in two steps: 1. A function `sdg_step` that calculates the gradients and performs the updates for one batch. 2. An outer loop that iterates through the training set and adjusts the learning rate."]},{"cell_type":"code","execution_count":64,"metadata":{"id":"hCprM5NcBU5-","executionInfo":{"status":"ok","timestamp":1682606634006,"user_tz":-120,"elapsed":18,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["# Performs one step of SGD.\n","def numpy_sdg_step(self, x, y, learning_rate):\n","    # Calculate the gradients\n","    dLdU, dLdV, dLdW = self.bptt(x, y)\n","    # Change parameters according to gradients and learning rate\n","    self.U -= learning_rate * dLdU\n","    self.V -= learning_rate * dLdV\n","    self.W -= learning_rate * dLdW\n","\n","RNNNumpy.sgd_step = numpy_sdg_step"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"cjVNoZNxBU5_","executionInfo":{"status":"ok","timestamp":1682606634006,"user_tz":-120,"elapsed":18,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["# Outer SGD Loop\n","# - model: The RNN model instance\n","# - X_train: The training data set\n","# - y_train: The training data labels\n","# - learning_rate: Initial learning rate for SGD\n","# - nepoch: Number of times to iterate through the complete dataset\n","# - evaluate_loss_after: Evaluate the loss after this many epochs\n","def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n","    # We keep track of the losses so we can plot them later\n","    losses = []\n","    num_examples_seen = 0\n","    for epoch in range(nepoch):\n","        # Optionally evaluate the loss\n","        if (epoch % evaluate_loss_after == 0):\n","            loss = model.calculate_loss(X_train, y_train)\n","            losses.append((num_examples_seen, loss))\n","            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n","            # Adjust the learning rate if loss increases\n","            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n","                learning_rate = learning_rate * 0.5  \n","                print(\"Setting learning rate to %f\" % learning_rate)\n","            sys.stdout.flush()\n","        # For each training example...\n","        for i in range(len(y_train)):\n","            # One SGD step\n","            # model.sgd_step(X_train[i], y_train[i], learning_rate)\n","            model.bptt_online_learning(X_train[i], y_train[i], learning_rate)\n","            num_examples_seen += 1\n","    return losses"]},{"cell_type":"markdown","metadata":{"id":"URMGCM8NBU5_"},"source":["Done! Let's try to get a sense of how long it would take to train our network:"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"Ipucg4gtBU5_","executionInfo":{"status":"ok","timestamp":1682606634006,"user_tz":-120,"elapsed":17,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"outputs":[],"source":["# np.random.seed(10)\n","# model = RNNNumpy(vocabulary_size)\n","# %timeit model.sgd_step(X_train[10], y_train[10], 0.005)"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"XwAB3sD8BU5_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682606900715,"user_tz":-120,"elapsed":266726,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"48bae315-9c47-4650-9179-57fff7c99dcc"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-60-3f0be9da26f3>:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n","  N = np.sum((len(y_i) for y_i in y))\n"]},{"output_type":"stream","name":"stdout","text":["2023-04-27 14:43:54: Loss after num_examples_seen=0 epoch=0: 8.987504\n","2023-04-27 14:44:23: Loss after num_examples_seen=100 epoch=1: 8.977820\n","2023-04-27 14:44:50: Loss after num_examples_seen=200 epoch=2: 8.963781\n","2023-04-27 14:45:16: Loss after num_examples_seen=300 epoch=3: 8.937943\n","2023-04-27 14:45:40: Loss after num_examples_seen=400 epoch=4: 8.885449\n","2023-04-27 14:46:06: Loss after num_examples_seen=500 epoch=5: 8.777299\n","2023-04-27 14:46:34: Loss after num_examples_seen=600 epoch=6: 8.613653\n","2023-04-27 14:47:00: Loss after num_examples_seen=700 epoch=7: 8.565121\n","2023-04-27 14:47:28: Loss after num_examples_seen=800 epoch=8: 8.391009\n","2023-04-27 14:47:54: Loss after num_examples_seen=900 epoch=9: 6.772534\n"]}],"source":["np.random.seed(10)\n","# Train on a small subset of the data to see what happens\n","model = RNNNumpy(vocabulary_size)\n","losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"]},{"cell_type":"code","source":["print(losses)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BKghGNe13Uw","executionInfo":{"status":"ok","timestamp":1682606900716,"user_tz":-120,"elapsed":22,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"262ca100-ef53-4a3c-f3c3-e712f2004942"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["[(0, 8.98750364771118), (100, 8.977820431808212), (200, 8.963781060314819), (300, 8.937943381303368), (400, 8.885448730255783), (500, 8.777299020831093), (600, 8.613653468212895), (700, 8.565120767327285), (800, 8.391009323946804), (900, 6.772533925718266)]\n"]}]},{"cell_type":"markdown","metadata":{"id":"NXK12dQsBU6A"},"source":["Good, it seems like our implementation is at least doing something useful and decreasing the loss, just like we wanted."]},{"cell_type":"code","source":[],"metadata":{"id":"v2gQeZKtfc41","executionInfo":{"status":"ok","timestamp":1682606900716,"user_tz":-120,"elapsed":7,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":68,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}