{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOc5MlzUormqHUpOlEZ/I5t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjESVaX0jx1s","executionInfo":{"status":"ok","timestamp":1686181880607,"user_tz":-120,"elapsed":2027,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"a26ce5cd-88b7-401c-9271-eb0d02615116"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","%cd /content/drive/MyDrive/graduation_project/FPTT-on-ANN/RNN/lstm-master/new/\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iTvpgbkij7I5","executionInfo":{"status":"ok","timestamp":1686181880608,"user_tz":-120,"elapsed":12,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"2b65a40b-d323-4f69-a86b-53411bdb11e8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/graduation_project/FPTT-on-ANN/RNN/lstm-master/new\n","/content/drive/MyDrive/graduation_project/FPTT-on-ANN/RNN/lstm-master/new\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import copy\n","import matplotlib.pyplot as plt\n","# import lstm as model\n","import lstm_no_b as model"],"metadata":{"id":"0mKtQRhQkGx4","executionInfo":{"status":"ok","timestamp":1686181880924,"user_tz":-120,"elapsed":321,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!python lstm.py"],"metadata":{"id":"jGD_Z6RZkHm5","executionInfo":{"status":"ok","timestamp":1686181881159,"user_tz":-120,"elapsed":236,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Load & Preprocess Data "],"metadata":{"id":"lSW9fnlzlLqH"}},{"cell_type":"code","source":["def LoadData(DirName,limit = 500):\n","    data = list();\n","    count = 0;\n","    with open(DirName) as f:\n","        for line in f:\n","            if count<limit:\n","                new_line = np.array(line.split(','))\n","                new_line = new_line.astype(np.float32)\n","                data.append(new_line)\n","                count += 1\n","    data = np.asarray(data)\n","    \n","    return data[:,0],data[:,1:]\n","\n","NoTrain = 60000\n","NoTest = 10000\n","Y_train,x_train = LoadData(\"../lab1-solution/data/mnist_train.csv\", NoTrain)\n","Y_test,x_test = LoadData(\"../lab1-solution/data/mnist_test.csv\", NoTest)\n","\n","print(f\"Test set size: {x_test.shape[0]} x {x_test.shape[1]}\")\n","print(f\"Train set size: {x_train.shape[0]} x {x_train.shape[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FE9aRkFDkZiQ","executionInfo":{"status":"ok","timestamp":1686181910656,"user_tz":-120,"elapsed":29499,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"4ff4c587-81b8-46f1-9350-a03bc927b3f7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Test set size: 10000 x 784\n","Train set size: 60000 x 784\n"]}]},{"cell_type":"code","source":["x_train_st = (x_train-np.average(x_train))/np.std(x_train)\n","x_test_st = (x_test-np.average(x_train))/np.std(x_train)"],"metadata":{"id":"R6zIS3aflffX","executionInfo":{"status":"ok","timestamp":1686181911101,"user_tz":-120,"elapsed":447,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# One-hot encoding train and test sets labels \n","y_train = np.zeros((Y_train.size, int(Y_train.max()) + 1))\n","# y_train = np.zeros((Y_train.size, 10))\n","y_train[np.arange(Y_train.size),Y_train.astype(int)] = 1.0;\n","\n","y_test  = np.zeros((Y_test.size, int(Y_test.max()) + 1))\n","# y_test  = np.zeros((Y_test.size, 10))\n","y_test[np.arange(Y_test.size),Y_test.astype(int)] = 1.0;\n","\n","print(f\"Your decimal label is {Y_train[0]:.0f} and your one-hot encoded label is {y_train[0,:]}\")\n","print(f\"Correct decimal label is 5 and correct one-hot encoded label is [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCZMCbo_lkg6","executionInfo":{"status":"ok","timestamp":1686181911101,"user_tz":-120,"elapsed":15,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"19782a34-43bb-4707-94f9-93b44a7af2f8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Your decimal label is 5 and your one-hot encoded label is [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","Correct decimal label is 5 and correct one-hot encoded label is [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"]}]},{"cell_type":"code","source":["X = x_train_st \n","Y = y_train "],"metadata":{"id":"uYD62je6lmqf","executionInfo":{"status":"ok","timestamp":1686181911101,"user_tz":-120,"elapsed":14,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Loss & Accuracy Definition"],"metadata":{"id":"DRjdToP86OID"}},{"cell_type":"code","source":["def cross_entropy(y_pred,y):\n","    \"\"\" Input Parameters: y_pred, y : array of float  Returns: c : float \"\"\"\n","    \n","    # Compute loss\n","    c = ((-np.log(y_pred))*y).sum(axis=1).sum()\n","\n","    print(y_pred, y, c)\n","    return c\n","\n","\n","def labeling(x):  \n","    # Set the label with the max probability to '1' and the rest to 0 \n","    label = np.zeros((x.shape[0],Y.shape[1]))\n","    label[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n","    return label\n","\n","def accuracy(y_pred,y):  \n","    # Calculate the accuracy along the rows, averaging the results over the number of samples.\n","    acc = np.all(y_pred==y,axis=1).mean()\n","    return acc"],"metadata":{"id":"05hx2xzFniVV","executionInfo":{"status":"ok","timestamp":1686181911102,"user_tz":-120,"elapsed":15,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Instantiate Model"],"metadata":{"id":"A2or17p2l7k_"}},{"cell_type":"code","source":["N_l = 128  # number of neurons in hidden layer\n","\n","layers = np.array([1] +[N_l]+[Y.shape[1]]) # \n","print(layers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_y2sgypiluB3","executionInfo":{"status":"ok","timestamp":1686181911102,"user_tz":-120,"elapsed":15,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"ecb112c3-e61e-4a59-b907-c6b8c9b0776a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[  1 128  10]\n"]}]},{"cell_type":"code","source":["np.random.seed(seed=0)"],"metadata":{"id":"suXAVgyFmNnP","executionInfo":{"status":"ok","timestamp":1686181911102,"user_tz":-120,"elapsed":13,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["param = model.LstmParam(layers)"],"metadata":{"id":"iEbagXC8pv32","executionInfo":{"status":"ok","timestamp":1686181911102,"user_tz":-120,"elapsed":13,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# def get_xt(p, step, T, inputs):\n","#     start = p*step\n","#     end = (p+1)*step\n","#     if (end >= T): end=T\n","        \n","#     x = inputs[ start : end ]\n","#     return x, start, end"],"metadata":{"id":"-YwGlbPuIDTN","executionInfo":{"status":"ok","timestamp":1686181911103,"user_tz":-120,"elapsed":14,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["seq_len = 784\n","k = 28 # p\n","step = seq_len / k\n"],"metadata":{"id":"ZbFwzy0Krh5k","executionInfo":{"status":"ok","timestamp":1686181911103,"user_tz":-120,"elapsed":14,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print(X.shape)\n","sample = 1;  # index of the image we want to show\n","\n","plt.title(f'The handwritten digit is {Y_train[sample]:.0f}')\n","\n","# Reshape the array into 28 x 28 array (2-dimensional array)\n","pixels = x_train[sample,:].reshape((28, 28))\n","plt.imshow(pixels, cmap='gray',vmin=0,vmax=1.0)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"_d9mGu9_asfF","executionInfo":{"status":"ok","timestamp":1686181911103,"user_tz":-120,"elapsed":13,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"f61d27d2-f1db-44a2-d865-5046e3ec3b2b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["Model = model.Lstm(784, 200, layers, param)\n","\n","for t in range(784):\n","    Model.state.forward(t, X[0:200, t:(t+1)] )\n","    # if t== (200||400||):\n","    #     print(t, Model.state.Y_hat[t])\n","print(600, Model.state.Y_hat[600])\n","print(783, Model.state.Y_hat[783])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buJz2lqFIaP0","executionInfo":{"status":"ok","timestamp":1686181914097,"user_tz":-120,"elapsed":3004,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"3188d3fe-c6f9-444e-870d-450d90b8774c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["600 [[0.09866299 0.10135393 0.09945475 ... 0.1000693  0.09967434 0.10139287]\n"," [0.09879626 0.10154654 0.09936927 ... 0.10005524 0.09941178 0.10164324]\n"," [0.10020738 0.09979338 0.10013594 ... 0.09997564 0.10005878 0.09974575]\n"," ...\n"," [0.09918113 0.10078144 0.0996711  ... 0.10006191 0.09984568 0.10081405]\n"," [0.0986673  0.10135401 0.09945376 ... 0.10006923 0.09967084 0.10139359]\n"," [0.10020734 0.09979342 0.10013592 ... 0.09997566 0.10005878 0.09974574]]\n","783 [[0.10020734 0.09979343 0.10013592 ... 0.09997566 0.10005878 0.09974574]\n"," [0.10020734 0.09979343 0.10013592 ... 0.09997566 0.10005878 0.09974574]\n"," [0.10020734 0.09979343 0.10013592 ... 0.09997566 0.10005878 0.09974574]\n"," ...\n"," [0.10020734 0.09979343 0.10013592 ... 0.09997566 0.10005878 0.09974574]\n"," [0.10020734 0.09979343 0.10013592 ... 0.09997566 0.10005878 0.09974574]\n"," [0.10020734 0.09979343 0.10013592 ... 0.09997566 0.10005878 0.09974574]]\n"]}]},{"cell_type":"code","source":["print(Model.param.l1_wf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5zHLcBVsb8x","executionInfo":{"status":"ok","timestamp":1686179069448,"user_tz":-120,"elapsed":16,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"d49d50e4-e15e-4c11-a962-4a99301cd57a"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.00862909  0.03804047  0.01816617 ... -0.08499544 -0.03507695\n","   0.02831495]\n"," [-0.03710939  0.02086238 -0.01259203 ... -0.07364121  0.00922641\n","   0.0149334 ]\n"," [ 0.08165959 -0.03674347 -0.04581543 ...  0.05670415  0.07227404\n","   0.05577726]\n"," ...\n"," [-0.02938182 -0.03493803 -0.07455167 ... -0.07899544 -0.0076339\n","   0.06084169]\n"," [ 0.08658275 -0.07437803 -0.07302671 ... -0.02045291 -0.08632061\n","   0.059842  ]\n"," [-0.02951533  0.06813321  0.03719318 ... -0.016658    0.0133768\n","  -0.03443887]]\n"]}]},{"cell_type":"code","source":["mean_train_loss_list = list()\n","train_acc_list = list()\n","\n","\n","# Initialize the batch size, the number of epochs, and the learning rate\n","n_samples = 10 #X.shape[0]\n","\n","batch_size = 2\n","epochs = 200\n","lr = 0.01\n","alpha = 0.1\n","\n","# Initialize the weights\n","# weights = init_weights(layers)\n","# w_g, w_lbd, w_rm = init_weights_derivatives(layers, weights)\n","\n","# Q: predicted label distribution of last training epoch --> used to make auxiliary loss (oracle loss)\n","Q = np.zeros_like(Y)\n","\n","target = np.zeros((batch_size, 10))\n","\n","\n","# Epoch for loop\n","for epoch in range(epochs):\n","\n","    if (epoch==50) or (epoch==75) or (epoch==90):\n","        lr *= 0.1\n","\n","    # Initialize the layers\n","    # ins, h, o = init_layer(layers,batch_size)\n","    # initiate hidden states for current batch\n","    Model = model.Lstm(seq_len, batch_size, layers, param)\n","   \n","    # Initialize the training loss and accuracy for each epoch\n","    train_loss = 0\n","    train_acc = 0\n","\n","    # Create a random permutation for shuffling\n","    shuffle = np.random.permutation(n_samples)\n","    print(shuffle)\n","\n","\n","    # Shuffle dataset and create mini-batches for each epoch\n","    X_batches = np.array_split(X[shuffle],n_samples/batch_size)\n","    Y_batches = np.array_split(Y[shuffle],n_samples/batch_size)\n","    Q_batches = np.array_split(Q[shuffle],n_samples/batch_size)\n","    target = np.empty_like(Q_batches)\n","\n","    # print(Y_batches)\n","\n","    # Mini-batch for loop\n","    for b in range(int(n_samples/batch_size)):\n","\n","        for p in range(28):\n","\n","            for t in range (28):\n","                cnt = p*28 + t            \n","                Model.state.forward(cnt, X_batches[b][:, cnt:(cnt+1)] )\n","            # print(cnt, Model.state.Y_hat[cnt])\n","\n","            # to make target for cross entropy and divergence term \n","            if epoch == 0:\n","                beta = 1\n","            else:\n","                beta = (cnt+1)/seq_len \n","            \n","            target[b] = beta*(Y_batches[b]) + (1-beta)*Q_batches[b]\n","\n","            # backwards - to calculate the gradients w.r.t. cross entropy loss and auxiliary loss\n","            Model.backward(cnt, target[b], trunc_h=27, trunc_s=27)\n","\n","            # update the weights by gradients obtained\n","            Model.param.apply_diff_fptt(lr, alpha)\n","\n","\n","        Q[shuffle[batch_size*b:batch_size*(b+1)]] = Q_batches[b]\n","\n","        train_loss += cross_entropy(Model.state.Y_hat[seq_len-1],Y_batches[b])\n","        train_acc += accuracy(labeling(Model.state.Y_hat[seq_len-1]),Y_batches[b])       \n","\n","\n","    mean_train_loss = train_loss/n_samples\n","    mean_train_loss_list.append(mean_train_loss)\n","    train_acc = (train_acc/len(X_batches))\n","    train_acc_list.append(train_acc)\n","\n","\n","\n","\n","    print(f\"Epoch {epoch+1}: train_loss = {mean_train_loss:.3f} | train_acc = {train_acc:.3f} \" )\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1686179660852,"user_tz":-120,"elapsed":591418,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"outputId":"7c5c6971-02de-4dae-a10c-6c90014a42b4","id":"AiVCiQRujVPl"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["[5 2 3 4 1 0 9 8 7 6]\n","[[0.10337754 0.09988921 0.10038095 0.09997839 0.09859343 0.0994251\n","  0.0988906  0.10030227 0.10016156 0.09900096]\n"," [0.10337754 0.09988921 0.10038095 0.09997839 0.09859343 0.0994251\n","  0.0988906  0.10030227 0.10016156 0.09900096]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.6155334815365805\n","[[0.10317011 0.10009999 0.10034874 0.0999635  0.09866019 0.09944889\n","  0.09894577 0.10026953 0.10014239 0.0989509 ]\n"," [0.10317011 0.10009999 0.10034874 0.0999635  0.09866019 0.09944889\n","  0.09894577 0.10026953 0.10014239 0.0989509 ]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.6147172063514486\n","[[0.10403065 0.09997792 0.10021066 0.09984738 0.09862574 0.09939908\n","  0.09889611 0.10012184 0.09998571 0.09890491]\n"," [0.10403065 0.09997792 0.10021066 0.09984738 0.09862574 0.09939908\n","  0.09889611 0.10012184 0.09998571 0.09890491]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.571682129405982\n","[[0.10384036 0.1002307  0.10017938 0.09984186 0.09849021 0.09942605\n","  0.09895146 0.1001011  0.09998274 0.09895614]\n"," [0.10384036 0.1002307  0.10017938 0.09984186 0.09849021 0.09942605\n","  0.09895146 0.1001011  0.09998274 0.09895614]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.61807889956763\n","[[0.10356699 0.10050947 0.10010919 0.09999924 0.09852092 0.09940796\n","  0.09895969 0.10003613 0.09993241 0.09895801]\n"," [0.10356699 0.10050947 0.10010919 0.09999924 0.09852092 0.09940796\n","  0.09895969 0.10003613 0.09993241 0.09895801]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.6000960581772885\n","Epoch 1: train_loss = 2.302 | train_acc = 0.100 \n","[5 1 9 2 6 7 0 3 4 8]\n","[[0.10389017 0.10041048 0.10016462 0.09993262 0.09853766 0.09936546\n","  0.09894959 0.09995061 0.09984734 0.09895144]\n"," [0.10389017 0.10041048 0.10016462 0.09993262 0.09853766 0.09936546\n","  0.09894959 0.09995061 0.09984734 0.09895144]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.56536124243568\n","[[0.10373633 0.10040119 0.10017791 0.09994988 0.09840263 0.09942034\n","  0.09902982 0.09997147 0.09988158 0.09902886]\n"," [0.10373633 0.10040119 0.10017791 0.09994988 0.09840263 0.09942034\n","  0.09902982 0.09997147 0.09988158 0.09902886]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.637375428783617\n","[[0.10349676 0.10052493 0.10013527 0.10002985 0.09845585 0.09943003\n","  0.09906133 0.09994493 0.09986498 0.09905607]\n"," [0.10349676 0.10052493 0.10013527 0.10002985 0.09845585 0.09943003\n","  0.09906133 0.09994493 0.09986498 0.09905607]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.599636138466092\n","[[0.10327173 0.10067585 0.10009657 0.10000714 0.09852962 0.09946228\n","  0.09909432 0.09992403 0.09985237 0.0990861 ]\n"," [0.10327173 0.10067585 0.10009657 0.10000714 0.09852962 0.09946228\n","  0.09909432 0.09992403 0.09985237 0.0990861 ]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.603826214451694\n","[[0.1030627  0.10084349 0.10006409 0.09998279 0.09860275 0.09947874\n","  0.09912983 0.09990859 0.09984483 0.09908219]\n"," [0.1030627  0.10084349 0.10006409 0.09998279 0.09860275 0.09947874\n","  0.09912983 0.09990859 0.09984483 0.09908219]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.605991128534415\n","Epoch 2: train_loss = 2.301 | train_acc = 0.100 \n","[5 2 7 4 1 0 6 8 9 3]\n","[[0.10288458 0.10080381 0.10017152 0.09998024 0.09856784 0.09951032\n","  0.09918031 0.0999124  0.09985802 0.09913095]\n"," [0.10288458 0.10080381 0.10017152 0.09998024 0.09856784 0.09951032\n","  0.09918031 0.0999124  0.09985802 0.09913095]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.6178815670013105\n","[[0.10270999 0.10074698 0.10016126 0.10007001 0.09864239 0.09953527\n","  0.09922387 0.09991238 0.09986309 0.09913476]\n"," [0.10270999 0.10074698 0.10016126 0.10007001 0.09864239 0.09953527\n","  0.09922387 0.09991238 0.09986309 0.09913476]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.613160306070418\n","[[0.10298051 0.10064833 0.10010134 0.1000217  0.09867873 0.09952992\n","  0.0992237  0.09986473 0.09981168 0.09913936]\n"," [0.10298051 0.10064833 0.10010134 0.1000217  0.09867873 0.09952992\n","  0.0992237  0.09986473 0.09981168 0.09913936]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.580512444509163\n","[[0.10279459 0.10098489 0.1000419  0.09997109 0.09871667 0.09951659\n","  0.09922524 0.09982522 0.09978024 0.09914357]\n"," [0.10279459 0.10098489 0.1000419  0.09997109 0.09871667 0.09951659\n","  0.09922524 0.09982522 0.09978024 0.09914357]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.585568763006748\n","[[0.10261221 0.10119339 0.10001577 0.09995198 0.09866928 0.09952987\n","  0.09925363 0.09981767 0.09978213 0.09917407]\n"," [0.10261221 0.10119339 0.10001577 0.09995198 0.09866928 0.09952987\n","  0.09925363 0.09981767 0.09978213 0.09917407]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.606703514131062\n","Epoch 3: train_loss = 2.300 | train_acc = 0.100 \n","[1 8 9 5 2 6 3 7 4 0]\n","[[0.10284989 0.10134044 0.09993745 0.09987955 0.09867011 0.09948231\n","  0.09922622 0.09974873 0.09971205 0.09915325]\n"," [0.10284989 0.10134044 0.09993745 0.09987955 0.09867011 0.09948231\n","  0.09922622 0.09974873 0.09971205 0.09915325]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.563754476524772\n","[[0.10271476 0.10127371 0.10002693 0.09988139 0.0986345  0.09951118\n","  0.09926821 0.09976048 0.09973055 0.09919829]\n"," [0.10271476 0.10127371 0.10002693 0.09988139 0.0986345  0.09951118\n","  0.09926821 0.09976048 0.09973055 0.09919829]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.618650066429384\n","[[0.10253866 0.10146629 0.10001078 0.0998699  0.09856811 0.09952642\n","  0.09929579 0.09975897 0.0997376  0.09922748]\n"," [0.10253866 0.10146629 0.10001078 0.0998699  0.09856811 0.09952642\n","  0.09929579 0.09975897 0.0997376  0.09922748]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.605036117205919\n","[[0.10234688 0.10167208 0.09996602 0.09990885 0.09860872 0.09951834\n","  0.09929811 0.0997331  0.09971758 0.09923031]\n"," [0.10234688 0.10167208 0.09996602 0.09990885 0.09860872 0.09951834\n","  0.09929811 0.0997331  0.09971758 0.09923031]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.589499514244935\n","[[0.10220514 0.10159193 0.09996332 0.09991449 0.09869503 0.0995618\n","  0.09933892 0.09974987 0.09973529 0.0992442 ]\n"," [0.10220514 0.10159193 0.09996332 0.09991449 0.09869503 0.0995618\n","  0.09933892 0.09974987 0.09973529 0.09924421]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.617148483784552\n","Epoch 4: train_loss = 2.299 | train_acc = 0.100 \n","[3 2 1 7 8 6 5 4 9 0]\n","[[0.10205244 0.10179925 0.09994055 0.09989746 0.09863343 0.09957156\n","  0.09935824 0.09974495 0.09973763 0.0992645 ]\n"," [0.10205244 0.10179925 0.09994055 0.09989746 0.09863343 0.09957156\n","  0.09935824 0.09974495 0.09973763 0.0992645 ]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.6010975643709\n","[[0.10226909 0.10166638 0.09989875 0.09992694 0.09866302 0.0995492\n","  0.09935168 0.09971111 0.09969895 0.09926488]\n"," [0.10226909 0.10166638 0.09989875 0.09992694 0.09866302 0.0995492\n","  0.09935168 0.09971111 0.09969895 0.09926488]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.583463801740394\n","[[0.10208554 0.10220407 0.09982288 0.09986114 0.09867354 0.09950636\n","  0.09931639 0.09965129 0.09964704 0.09923175]\n"," [0.10208554 0.10220407 0.09982288 0.09986114 0.09867354 0.09950636\n","  0.09931639 0.09965129 0.09964704 0.09923175]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.56156757392894\n","[[0.10194988 0.10211858 0.09988807 0.0998576  0.09874651 0.09952842\n","  0.09934728 0.09966381 0.09966094 0.09923891]\n"," [0.10194988 0.10211858 0.09988807 0.0998576  0.09874651 0.09952842\n","  0.09934728 0.09966381 0.09966094 0.09923891]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.6139301269419555\n","[[0.10184161 0.10200092 0.09990743 0.09987235 0.09872152 0.09957984\n","  0.09939515 0.09969583 0.0996954  0.09928994]\n"," [0.10184161 0.10200092 0.09990743 0.09987235 0.09872152 0.09957984\n","  0.09939515 0.09969583 0.0996954  0.09928994]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.622247821572271\n","Epoch 5: train_loss = 2.298 | train_acc = 0.300 \n","[0 9 2 3 5 6 7 1 8 4]\n","[[0.10174162 0.10189207 0.09992295 0.0998887  0.09867859 0.09963285\n","  0.09944239 0.09972817 0.09973036 0.0993423 ]\n"," [0.10174162 0.10189207 0.09992295 0.0998887  0.09867859 0.09963285\n","  0.09944239 0.09972817 0.09973036 0.0993423 ]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.622150634345141\n","[[0.10161419 0.10211204 0.09990256 0.09987337 0.09859118 0.09963747\n","  0.09945458 0.09972379 0.09973251 0.09935831]\n"," [0.10161419 0.10211204 0.09990256 0.09987337 0.09859118 0.09963747\n","  0.09945458 0.09972379 0.09973251 0.09935831]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.59845812021052\n","[[0.1014679  0.10235038 0.09991785 0.09983465 0.09862379 0.09961698\n","  0.09944093 0.09969316 0.09970585 0.09934852]\n"," [0.1014679  0.10235038 0.09991785 0.09983465 0.09862379 0.09961698\n","  0.09944093 0.09969316 0.09970585 0.09934852]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.582760220519184\n","[[0.10163933 0.10219965 0.09989016 0.09985817 0.09867208 0.09960089\n","  0.09943731 0.09967202 0.09967889 0.09935149]\n"," [0.10163933 0.10219965 0.09989016 0.09985817 0.09867208 0.09960089\n","  0.09943731 0.09967202 0.09967889 0.09935149]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.590329028524238\n","[[0.10152032 0.10242534 0.09985214 0.09982936 0.09871646 0.0995876\n","  0.09943131 0.0996514  0.09966076 0.09932532]\n"," [0.10152032 0.10242534 0.09985214 0.09982936 0.09871646 0.0995876\n","  0.09943131 0.0996514  0.09966076 0.09932532]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.587975845235489\n","Epoch 6: train_loss = 2.298 | train_acc = 0.300 \n","[7 8 9 1 4 5 3 6 0 2]\n","[[0.1013742  0.10269119 0.09980588 0.09984216 0.09874578 0.09956437\n","  0.09941372 0.09962007 0.09963315 0.09930947]\n"," [0.1013742  0.10269119 0.09980588 0.09984216 0.09874578 0.09956437\n","  0.09941372 0.09962007 0.09963315 0.09930947]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.580193688223055\n","[[0.10156153 0.1025439  0.09979715 0.09983388 0.09868349 0.09956762\n","  0.09942834 0.09962109 0.09963112 0.09933189]\n"," [0.10156153 0.1025439  0.09979715 0.09983388 0.09868349 0.09956762\n","  0.09942834 0.09962109 0.09963112 0.09933189]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.602928095629998\n","[[0.10148262 0.1023879  0.09985458 0.09983776 0.09875267 0.09959079\n","  0.09945815 0.09964152 0.09965039 0.09934364]\n"," [0.10148262 0.1023879  0.09985458 0.09983776 0.09875267 0.09959079\n","  0.09945815 0.09964152 0.09965039 0.09934364]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.613210743814918\n","[[0.10130016 0.10305034 0.09977003 0.09975874 0.098742   0.09953173\n","  0.09940098 0.09957044 0.09958755 0.09928803]\n"," [0.10130016 0.10305034 0.09977003 0.09975874 0.098742   0.09953173\n","  0.09940098 0.09957044 0.09958755 0.09928803]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.545075341861062\n","[[0.10122185 0.10294741 0.09978506 0.09977317 0.09870786 0.09957382\n","  0.09943876 0.09960121 0.0996191  0.09933176]\n"," [0.10122185 0.10294741 0.09978506 0.09977317 0.09870786 0.09957382\n","  0.09943876 0.09960121 0.0996191  0.09933176]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.622446708131705\n","Epoch 7: train_loss = 2.296 | train_acc = 0.300 \n","[1 2 5 0 8 3 7 9 6 4]\n","[[0.10140208 0.10276183 0.09978528 0.09977222 0.09863689 0.09958474\n","  0.09945929 0.09961144 0.09962587 0.09936036]\n"," [0.10140208 0.10276183 0.09978528 0.09977222 0.09863689 0.09958474\n","  0.09945929 0.09961144 0.09962587 0.09936036]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.604971603194665\n","[[0.1013275  0.10259104 0.09983609 0.09977702 0.09870581 0.09961503\n","  0.09948427 0.09962985 0.0996425  0.09939089]\n"," [0.1013275  0.10259104 0.09983609 0.09977702 0.09870581 0.09961503\n","  0.09948427 0.09962985 0.0996425  0.09939089]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.61066773276459\n","[[0.10114903 0.10328583 0.09974708 0.09969706 0.09869319 0.0995503\n","  0.09942018 0.09955383 0.09957473 0.09932879]\n"," [0.10114903 0.10328583 0.09974708 0.09969706 0.09869319 0.0995503\n","  0.09942018 0.09955383 0.09957473 0.09932879]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.540510157165658\n","[[0.10107704 0.10317237 0.09976229 0.09975255 0.09864845 0.09957807\n","  0.09945423 0.09958306 0.09960513 0.0993668 ]\n"," [0.10107704 0.10317237 0.09976229 0.09975255 0.09864845 0.09957807\n","  0.09945423 0.09958306 0.09960513 0.0993668 ]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.621255340839842\n","[[0.10095908 0.10345706 0.09972232 0.09972006 0.09867442 0.09955678\n","  0.09943705 0.09955766 0.09958214 0.09933343]\n"," [0.10095908 0.10345705 0.09972232 0.09972006 0.09867442 0.09955678\n","  0.09943705 0.09955766 0.09958214 0.09933343]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.577871743012759\n","Epoch 8: train_loss = 2.296 | train_acc = 0.300 \n","[9 7 8 1 0 6 5 2 3 4]\n","[[0.10090232 0.10330263 0.09974355 0.09977953 0.09862947 0.09958786\n","  0.09947413 0.09959084 0.0996157  0.09937397]\n"," [0.10090232 0.10330263 0.09974355 0.09977953 0.09862947 0.09958786\n","  0.09947413 0.09959084 0.0996157  0.09937397]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.621177467068922\n","[[0.10098993 0.10356842 0.09968155 0.09972178 0.09863108 0.09953954\n","  0.09943151 0.09953747 0.09956143 0.09933729]\n"," [0.10098993 0.10356842 0.09968155 0.09972178 0.09863108 0.09953954\n","  0.09943151 0.09953747 0.09956143 0.09933729]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.560257283409575\n","[[0.10087833 0.10391131 0.0996317  0.09967354 0.09865281 0.09951629\n","  0.09940068 0.09949936 0.09952595 0.09931003]\n"," [0.10087833 0.10391131 0.0996317  0.09967354 0.09865281 0.09951629\n","  0.09940068 0.09949936 0.09952595 0.09931003]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.571651540893885\n","[[0.1008255  0.10373773 0.09968933 0.09969517 0.0986126  0.09955011\n","  0.09943906 0.09953523 0.09956194 0.09935332]\n"," [0.1008255  0.10373773 0.09968933 0.09969517 0.0986126  0.09955011\n","  0.09943906 0.09953523 0.09956194 0.09935332]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.622252799679986\n","[[0.10071528 0.10405362 0.09964899 0.09965637 0.09863304 0.09952344\n","  0.09941531 0.099505   0.09953417 0.09931479]\n"," [0.10071528 0.10405362 0.09964899 0.09965637 0.09863304 0.09952344\n","  0.09941531 0.099505   0.09953417 0.09931479]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.572309746940002\n","Epoch 9: train_loss = 2.295 | train_acc = 0.300 \n","[4 9 6 2 1 8 5 7 0 3]\n","[[0.10067819 0.10388053 0.099679   0.09968449 0.09859402 0.0995609\n","  0.09945888 0.09954661 0.09957508 0.09934228]\n"," [0.10067819 0.10388053 0.099679   0.09968449 0.09859402 0.0995609\n","  0.09945888 0.09954661 0.09957508 0.09934228]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.625928669294754\n","[[0.10058933 0.10421556 0.099649   0.09965866 0.09847777 0.09954422\n","  0.09944503 0.0995272  0.09956143 0.0993318 ]\n"," [0.10058933 0.10421556 0.099649   0.09965866 0.09847777 0.09954422\n","  0.09944503 0.0995272  0.09956143 0.0993318 ]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.579218236285298\n","[[0.10064348 0.10457865 0.09958007 0.09959387 0.09847569 0.09948732\n","  0.09939179 0.09946509 0.09949928 0.09928475]\n"," [0.10064348 0.10457865 0.09958007 0.09959387 0.09847569 0.09948732\n","  0.09939179 0.09946509 0.09949928 0.09928475]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.553986718502818\n","[[0.10059935 0.1043606  0.09961832 0.09963133 0.09855722 0.09950538\n","  0.09941382 0.09948558 0.09951567 0.09931272]\n"," [0.10059935 0.1043606  0.09961832 0.09963133 0.09855722 0.09950538\n","  0.09941382 0.09948558 0.09951567 0.09931272]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.612687760654055\n","[[0.1004913  0.10470428 0.09957068 0.09958729 0.09857944 0.09947902\n","  0.09937992 0.09944646 0.09947887 0.09928274]\n"," [0.1004913  0.10470428 0.09957068 0.09958729 0.09857944 0.09947902\n","  0.09937992 0.09944646 0.09947887 0.09928274]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.564423824130204\n","Epoch 10: train_loss = 2.294 | train_acc = 0.300 \n","[4 6 1 8 9 5 7 2 3 0]\n","[[0.10038391 0.10511762 0.09951651 0.09953763 0.09859333 0.09943974\n","  0.09934159 0.09940245 0.09943765 0.09922958]\n"," [0.10038391 0.10511762 0.09951651 0.09953763 0.09859333 0.09943974\n","  0.09934159 0.09940245 0.09943765 0.09922958]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.562994519381366\n","[[0.10042531 0.10553633 0.09944365 0.09946895 0.09858233 0.09937784\n","  0.09928281 0.09933584 0.09937184 0.09917511]\n"," [0.10042531 0.10553633 0.09944365 0.09946895 0.09858233 0.09937784\n","  0.09928281 0.09933584 0.09937184 0.09917511]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.5470410354794994\n","[[0.10040594 0.10530064 0.09949889 0.09949801 0.09855012 0.09941344\n","  0.09932314 0.09937681 0.09941126 0.09922177]\n"," [0.10040594 0.10530064 0.09949889 0.09949801 0.09855012 0.09941344\n","  0.09932314 0.09937681 0.09941126 0.09922177]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.624798868931264\n","[[0.10038486 0.105028   0.09953938 0.09955832 0.09850678 0.09945532\n","  0.09937031 0.09942491 0.09945738 0.09927473]\n"," [0.10038486 0.105028   0.09953938 0.09955832 0.09850678 0.09945532\n","  0.09937031 0.09942491 0.09945738 0.09927473]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.624641498045344\n","[[0.10028351 0.10540508 0.09948768 0.09951206 0.09851493 0.09942478\n","  0.09933218 0.09938227 0.09941739 0.09924012]\n"," [0.10028351 0.10540508 0.09948768 0.09951206 0.09851493 0.09942478\n","  0.09933218 0.09938227 0.09941739 0.09924012]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.558298348014267\n","Epoch 11: train_loss = 2.292 | train_acc = 0.300 \n","[5 9 4 1 6 7 8 3 2 0]\n","[[0.1002643  0.10517636 0.09954301 0.09954219 0.09847096 0.09946191\n","  0.09937304 0.09942398 0.09945797 0.09928626]\n"," [0.1002643  0.10517636 0.09954301 0.09954219 0.09847096 0.09946191\n","  0.09937304 0.09942398 0.09945797 0.09928626]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.625158987415692\n","[[0.10037186 0.10488424 0.09955856 0.09955299 0.09854124 0.09947816\n","  0.09939607 0.09944528 0.09947171 0.0992999 ]\n"," [0.10037186 0.10488424 0.09955856 0.09955299 0.09854124 0.09947816\n","  0.09939607 0.09944528 0.09947171 0.0992999 ]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.608484099401789\n","[[0.10028023 0.10525001 0.09950486 0.09952668 0.09855591 0.09943676\n","  0.09935523 0.09940001 0.09942914 0.09926118]\n"," [0.10028023 0.10525001 0.09950486 0.09952668 0.09855591 0.09943676\n","  0.09935523 0.09940001 0.09942914 0.09926118]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.558746230109361\n","[[0.10009885 0.10641992 0.09936007 0.09939706 0.09848085 0.09931379\n","  0.09922809 0.09926265 0.09930398 0.09913475]\n"," [0.10009885 0.10641992 0.09936007 0.09939706 0.09848085 0.09931379\n","  0.09922809 0.09926265 0.09930398 0.09913475]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.480725058040061\n","[[0.10008171 0.10621294 0.09939195 0.09942419 0.09844341 0.0993561\n","  0.09926652 0.09930274 0.09934172 0.09917871]\n"," [0.10008171 0.10621294 0.09939195 0.09942419 0.09844341 0.0993561\n","  0.09926652 0.09930274 0.09934172 0.09917871]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.6273182472522985\n","Epoch 12: train_loss = 2.290 | train_acc = 0.300 \n","[0 5 7 1 4 8 6 2 9 3]\n","[[0.10006425 0.10588918 0.09943972 0.09944894 0.09852327 0.09939766\n","  0.0993025  0.0993405  0.09937372 0.09922026]\n"," [0.10006425 0.10588918 0.09943972 0.09944894 0.09852327 0.09939766\n","  0.0993025  0.0993405  0.09937372 0.09922026]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.616830322941272\n","[[0.10015962 0.10556532 0.09945762 0.09948186 0.09859536 0.09941469\n","  0.09932514 0.09936289 0.09938878 0.09924872]\n"," [0.10015962 0.10556532 0.09945762 0.09948186 0.09859536 0.09941469\n","  0.09932514 0.09936289 0.09938878 0.09924872]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.608770131111431\n","[[0.10007592 0.10596707 0.09940478 0.09943447 0.09860163 0.09937188\n","  0.09928405 0.09931742 0.09934619 0.0991966 ]\n"," [0.10007592 0.10596707 0.09940478 0.09943447 0.09860163 0.09937188\n","  0.09928405 0.09931742 0.09934619 0.0991966 ]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.555278464642406\n","[[0.09998905 0.10645398 0.09935491 0.09938921 0.09849363 0.09933166\n","  0.09924478 0.09927426 0.09930959 0.09915893]\n"," [0.09998905 0.10645398 0.09935491 0.09938921 0.09849363 0.09933166\n","  0.09924478 0.09927426 0.09930959 0.09915893]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.55780593766357\n","[[0.0999043  0.10696351 0.09930476 0.09934382 0.09836569 0.0992906\n","  0.09920418 0.09923    0.09927244 0.09912072]\n"," [0.0999043  0.10696351 0.09930476 0.09934382 0.09836569 0.0992906\n","  0.09920418 0.09923    0.09927244 0.09912072]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.554330801245696\n","Epoch 13: train_loss = 2.289 | train_acc = 0.300 \n","[7 8 6 5 0 3 2 4 1 9]\n","[[0.09980494 0.10747533 0.09923788 0.09930122 0.09835274 0.09923308\n","  0.09914684 0.0991682  0.09921416 0.09906562]\n"," [0.09980494 0.10747533 0.09923788 0.09930122 0.09835274 0.09923308\n","  0.09914684 0.0991682  0.09921416 0.09906562]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.54009133368993\n","[[0.09970632 0.10800431 0.09918094 0.09923727 0.09834424 0.09917374\n","  0.09908714 0.09910418 0.09915373 0.09900812]\n"," [0.09970632 0.10800431 0.09918094 0.09923727 0.09834424 0.09917374\n","  0.09908714 0.09910418 0.09915373 0.09900812]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.536393517778168\n","[[0.09960926 0.10854909 0.09911141 0.09917024 0.09833116 0.09912263\n","  0.09902619 0.09903901 0.09909171 0.0989493 ]\n"," [0.09960926 0.10854909 0.09911141 0.09917024 0.09833116 0.09912263\n","  0.09902619 0.09903901 0.09909171 0.0989493 ]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.531950319501556\n","[[0.09962005 0.10823342 0.09915809 0.09920977 0.0983114  0.09916688\n","  0.09907618 0.09909257 0.09914051 0.09899113]\n"," [0.09962005 0.10823342 0.09915809 0.09920977 0.0983114  0.09916688\n","  0.09907618 0.09909257 0.09914051 0.09899113]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.632340287233173\n","[[0.09972072 0.10785392 0.0992039  0.09924825 0.09826953 0.09920759\n","  0.09912371 0.09914299 0.09918561 0.09904376]\n"," [0.09972072 0.10785392 0.0992039  0.09924825 0.09826953 0.09920759\n","  0.09912371 0.09914299 0.09918561 0.09904376]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.625423012266667\n","Epoch 14: train_loss = 2.287 | train_acc = 0.300 \n","[1 4 7 5 2 3 8 6 9 0]\n","[[0.09981962 0.10746958 0.0992352  0.09927245 0.09834846 0.09923506\n","  0.09915783 0.09917871 0.09921266 0.09907042]\n"," [0.09981962 0.10746958 0.0992352  0.09927245 0.09834846 0.09923506\n","  0.09915783 0.09917871 0.09921266 0.09907042]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.616314893184418\n","[[0.0998192  0.1071014  0.09928101 0.09931756 0.09843815 0.09926853\n","  0.09919541 0.09921928 0.09924713 0.09911233]\n"," [0.0998192  0.1071014  0.09928101 0.09931756 0.09843815 0.09926853\n","  0.09919541 0.09921928 0.09924713 0.09911233]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.619233837668586\n","[[0.09974382 0.10757069 0.09923519 0.0992762  0.09832289 0.09922972\n","  0.09915667 0.09917714 0.09921159 0.09907609]\n"," [0.09974382 0.10757069 0.09923519 0.0992762  0.09832289 0.09922972\n","  0.09915667 0.09917714 0.09921159 0.09907609]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.549105502922853\n","[[0.09955929 0.10898448 0.0990641  0.09912059 0.09820995 0.0990783\n","  0.09899972 0.09900817 0.09905687 0.09891853]\n"," [0.09955929 0.10898448 0.0990641  0.09912059 0.09820995 0.0990783\n","  0.09899972 0.09900817 0.09905687 0.09891853]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.433099644150201\n","[[0.09956334 0.10872475 0.0991041  0.09915411 0.09817109 0.09912366\n","  0.0990413  0.09905353 0.09909904 0.09896509]\n"," [0.09956334 0.10872475 0.0991041  0.09915411 0.09817109 0.09912366\n","  0.0990413  0.09905353 0.09909904 0.09896509]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.632430683993635\n","Epoch 15: train_loss = 2.285 | train_acc = 0.300 \n","[4 0 1 7 6 3 2 5 9 8]\n","[[0.09957142 0.10832653 0.09914716 0.09918921 0.09826264 0.09917245\n","  0.09908623 0.09910215 0.09913974 0.09900249]\n"," [0.09957142 0.10832653 0.09914716 0.09918921 0.09826264 0.09917245\n","  0.09908623 0.09910215 0.09913974 0.09900249]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.62350533581828\n","[[0.09965098 0.10792761 0.09917934 0.09922832 0.09834829 0.09920052\n","  0.09911936 0.09913773 0.09916732 0.09904052]\n"," [0.09965098 0.10792761 0.09917934 0.09922832 0.09834829 0.09920052\n","  0.09911936 0.09913773 0.09916732 0.09904052]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.616413202939338\n","[[0.09947858 0.10929791 0.09901276 0.09907737 0.09824342 0.09905139\n","  0.09896532 0.09897194 0.09901549 0.09888582]\n"," [0.09947858 0.10929791 0.09901276 0.09907737 0.09824342 0.09905139\n","  0.09896532 0.09897194 0.09901549 0.09888582]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.427356051941689\n","[[0.0994847  0.10903261 0.09905907 0.09911126 0.09821203 0.09908769\n","  0.0990064  0.09901694 0.09905761 0.09893169]\n"," [0.0994847  0.10903261 0.09905907 0.09911126 0.09821203 0.09908769\n","  0.0990064  0.09901694 0.09905761 0.09893169]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.632665466485444\n","[[0.09941567 0.10954671 0.09901049 0.09906601 0.09807651 0.09904364\n","  0.09896304 0.09897017 0.09901739 0.09889037]\n"," [0.09941567 0.10954671 0.09901049 0.09906601 0.09807651 0.09904364\n","  0.09896304 0.09897017 0.09901739 0.09889037]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.5334116674630955\n","Epoch 16: train_loss = 2.283 | train_acc = 0.300 \n","[4 1 7 6 5 2 0 3 9 8]\n","[[0.0994916  0.1091915  0.09904176 0.09908979 0.0981554  0.09906995\n","  0.09899522 0.09900504 0.09904402 0.09891573]\n"," [0.0994916  0.1091915  0.09904176 0.09908979 0.0981554  0.09906995\n","  0.09899522 0.09900504 0.09904402 0.09891573]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.6211691233291425\n","[[0.09940981 0.10968907 0.09897686 0.09904183 0.09815032 0.09901139\n","  0.0989363  0.09894216 0.09898421 0.09885805]\n"," [0.09940981 0.10968907 0.09897686 0.09904183 0.09815032 0.09901139\n","  0.0989363  0.09894216 0.09898421 0.09885805]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.522318502361554\n","[[0.09942608 0.10935156 0.09903138 0.09908514 0.09812051 0.09905624\n","  0.09898572 0.09899617 0.09903475 0.09891244]\n"," [0.09942608 0.10935156 0.09903138 0.09908514 0.09812051 0.09905624\n","  0.09898572 0.09899617 0.09903475 0.09891244]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.633877313338125\n","[[0.09934392 0.10985262 0.09896808 0.0990246  0.09810854 0.09900661\n","  0.09892799 0.09893453 0.09897638 0.09885673]\n"," [0.09934392 0.10985262 0.09896808 0.0990246  0.09810854 0.09900661\n","  0.09892799 0.09893453 0.09897638 0.09885673]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.521184353436356\n","[[0.09926976 0.11044402 0.09890921 0.09897027 0.09797098 0.09895375\n","  0.09887443 0.09887692 0.09892588 0.09880476]\n"," [0.09926976 0.11044402 0.09890921 0.09897027 0.09797098 0.09895375\n","  0.09887443 0.09887692 0.09892588 0.09880476]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.526330396703129\n","Epoch 17: train_loss = 2.282 | train_acc = 0.300 \n","[9 6 1 2 8 7 5 3 4 0]\n","[[0.09919896 0.11104179 0.09885225 0.09891776 0.09781299 0.09890154\n","  0.09882237 0.09882103 0.09887714 0.09875416]\n"," [0.09919896 0.11104179 0.09885225 0.09891776 0.09781299 0.09890154\n","  0.09882237 0.09882103 0.09887714 0.09875416]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.522546500051792\n","[[0.09928463 0.11068684 0.09890231 0.09895984 0.09774333 0.09894433\n","  0.09887106 0.09887425 0.09892565 0.09880777]\n"," [0.09928463 0.11068684 0.09890231 0.09895984 0.09774333 0.09894433\n","  0.09887106 0.09887425 0.09892565 0.09880777]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.635174863287595\n","[[0.09920592 0.11119418 0.09883612 0.09890897 0.0977421  0.09888345\n","  0.09880987 0.09880897 0.09886251 0.09874791]\n"," [0.09920592 0.11119418 0.09883612 0.09890897 0.0977421  0.09888345\n","  0.09880987 0.09880897 0.09886251 0.09874791]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.5100325660694285\n","[[0.09911714 0.11177212 0.09876407 0.09883956 0.09774219 0.09881587\n","  0.09874094 0.09873584 0.09879241 0.09867985]\n"," [0.09911714 0.11177212 0.09876407 0.09883956 0.09774219 0.09881587\n","  0.09874094 0.09873584 0.09879241 0.09867985]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.506314557123976\n","[[0.09912669 0.11139764 0.09880338 0.09886929 0.09784645 0.09885748\n","  0.09877969 0.09877937 0.0988272  0.09871281]\n"," [0.09912669 0.11139764 0.09880338 0.09886929 0.09784645 0.09885748\n","  0.09877969 0.09877937 0.0988272  0.09871281]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.629616660774944\n","Epoch 18: train_loss = 2.280 | train_acc = 0.300 \n","[2 4 3 7 1 6 5 9 0 8]\n","[[0.09916202 0.11097446 0.09886675 0.09892369 0.09781355 0.09891392\n","  0.09884164 0.09884689 0.09888958 0.0987675 ]\n"," [0.09916202 0.11097446 0.09886675 0.09892369 0.09781355 0.09891392\n","  0.09884164 0.09884689 0.09888958 0.0987675 ]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.639678809441461\n","[[0.09908503 0.11148236 0.09880137 0.09887293 0.09780682 0.09885368\n","  0.09878077 0.09878192 0.09882757 0.09870755]\n"," [0.09908503 0.11148236 0.09880137 0.09887293 0.09780682 0.09885368\n","  0.09878077 0.09878192 0.09882757 0.09870755]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.50780873530815\n","[[0.09905188 0.11205407 0.09872301 0.0987988  0.09779529 0.09878028\n","  0.09870695 0.09870276 0.09875101 0.09863595]\n"," [0.09905188 0.11205407 0.09872301 0.0987988  0.09779529 0.09878028\n","  0.09870695 0.09870276 0.09875101 0.09863595]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.500885212365616\n","[[0.09908132 0.11169673 0.09877831 0.09884472 0.0977537  0.09882786\n","  0.09875902 0.09876059 0.09880521 0.09869254]\n"," [0.09908132 0.11169673 0.09877831 0.09884472 0.0977537  0.09882786\n","  0.09875902 0.09876059 0.09880521 0.09869254]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.640181399604333\n","[[0.09900585 0.11220262 0.09871396 0.09878276 0.09774697 0.09877525\n","  0.09869883 0.0986962  0.09874331 0.09863424]\n"," [0.09900585 0.11220262 0.09871396 0.09878276 0.09774697 0.09877525\n","  0.09869883 0.0986962  0.09874331 0.09863424]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.502357116631587\n","Epoch 19: train_loss = 2.279 | train_acc = 0.300 \n","[2 7 9 4 0 1 3 8 5 6]\n","[[0.09903346 0.11184382 0.09876791 0.09883958 0.0977014  0.0988227\n","  0.09875066 0.09875353 0.0987967  0.09869024]\n"," [0.09903346 0.11184382 0.09876791 0.09883958 0.0977014  0.0988227\n","  0.09875066 0.09875353 0.0987967  0.09869024]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.640096578420245\n","[[0.09907499 0.11142102 0.09883425 0.0988976  0.09764235 0.09888092\n","  0.0988148  0.0988232  0.09886235 0.09874854]\n"," [0.09907499 0.11142102 0.09883425 0.0988976  0.09764235 0.09888092\n","  0.0988148  0.0988232  0.09886235 0.09874854]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.641622665494005\n","[[0.09913868 0.11097276 0.09887624 0.09893016 0.0977454  0.09892283\n","  0.09885511 0.09886758 0.09889817 0.09879308]\n"," [0.09913868 0.11097276 0.09887624 0.09893016 0.0977454  0.09892283\n","  0.09885511 0.09886758 0.09889817 0.09879308]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.62465083884205\n","[[0.0989712  0.11242899 0.09869785 0.0987675  0.09764851 0.098761\n","  0.09868685 0.0986852  0.09872975 0.09862316]\n"," [0.0989712  0.11242899 0.09869785 0.0987675  0.09764851 0.098761\n","  0.09868685 0.0986852  0.09872975 0.09862316]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.370866953565936\n","[[0.09888408 0.11306625 0.09861706 0.09869162 0.09763931 0.09868623\n","  0.09861081 0.09860398 0.09865258 0.09854809]\n"," [0.09888408 0.11306625 0.09861706 0.09869162 0.09763931 0.09868623\n","  0.09861081 0.09860398 0.09865258 0.09854809]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.4962923818905205\n","Epoch 20: train_loss = 2.277 | train_acc = 0.300 \n","[2 4 1 3 8 0 7 5 6 9]\n","[[0.09891601 0.11271041 0.09867325 0.09873906 0.09759427 0.0987345\n","  0.09866484 0.09866359 0.09870784 0.09859623]\n"," [0.09891601 0.11271041 0.09867325 0.09873906 0.09759427 0.0987345\n","  0.09866484 0.09866359 0.09870784 0.09859623]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.643658728227756\n","[[0.09888757 0.11320184 0.09860631 0.09867495 0.09758658 0.09867107\n","  0.09860104 0.09859532 0.09864171 0.09853361]\n"," [0.09888757 0.11320184 0.09860631 0.09867495 0.09758658 0.09867107\n","  0.09860104 0.09859532 0.09864171 0.09853361]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.492354626252199\n","[[0.09881136 0.1137555  0.09853491 0.09860663 0.09758444 0.09861154\n","  0.09853323 0.09852287 0.09857182 0.09846769]\n"," [0.09881136 0.1137555  0.09853491 0.09860663 0.09758444 0.09861154\n","  0.09853323 0.09852287 0.09857182 0.09846769]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.49027085213447\n","[[0.09882315 0.11338256 0.09857237 0.09864629 0.0976823  0.09864446\n","  0.09856947 0.0985649  0.09860649 0.09850802]\n"," [0.09882315 0.11338256 0.09857237 0.09864629 0.0976823  0.09864446\n","  0.09856947 0.0985649  0.09860649 0.09850802]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.633179000077815\n","[[0.09876956 0.11388452 0.09852565 0.09860304 0.09754597 0.09859938\n","  0.098525   0.09851712 0.0985648  0.09846497]\n"," [0.09876956 0.11388452 0.09852565 0.09860304 0.09754597 0.09859938\n","  0.098525   0.09851712 0.0985648  0.09846497]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.500001842651761\n","Epoch 21: train_loss = 2.276 | train_acc = 0.300 \n","[9 0 2 4 1 5 3 7 6 8]\n","[[0.09880149 0.11353323 0.0985814  0.09864984 0.09748402 0.09865503\n","  0.09857759 0.09857579 0.09862002 0.0985216 ]\n"," [0.09880149 0.11353323 0.0985814  0.09864984 0.09748402 0.09865503\n","  0.09857759 0.09857579 0.09862002 0.0985216 ]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.644192883171576\n","[[0.0988455  0.11311312 0.09864805 0.09870731 0.09741988 0.09871319\n","  0.09864142 0.09864561 0.09868566 0.09858027]\n"," [0.0988455  0.11311312 0.09864805 0.09870731 0.09741988 0.09871319\n","  0.09864142 0.09864561 0.09868566 0.09858027]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.645609106482937\n","[[0.0989037  0.11266341 0.09869106 0.09874234 0.09752641 0.09875001\n","  0.0986826  0.09869173 0.09872368 0.09862506]\n"," [0.0989037  0.11266341 0.09869106 0.09874234 0.09752641 0.09875001\n","  0.0986826  0.09869173 0.09872368 0.09862506]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.629369473548432\n","[[0.09883462 0.11315636 0.09862539 0.09868787 0.09753883 0.09868854\n","  0.09862033 0.09862472 0.09865956 0.09856379]\n"," [0.09883462 0.11315636 0.09862539 0.09868787 0.09753883 0.09868854\n","  0.09862033 0.09862472 0.09865956 0.09856379]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.494778019881412\n","[[0.09867202 0.11461743 0.09844737 0.09852616 0.09743866 0.09852464\n","  0.09845125 0.09844058 0.09848888 0.098393  ]\n"," [0.09867202 0.11461743 0.09844737 0.09852616 0.09743866 0.09852464\n","  0.09845125 0.09844058 0.09848888 0.098393  ]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.332310722256593\n","Epoch 22: train_loss = 2.275 | train_acc = 0.300 \n","[5 1 4 0 6 7 2 3 9 8]\n","[[0.09871694 0.11431629 0.09847185 0.09854462 0.09752847 0.09854525\n","  0.09847542 0.09846945 0.09851081 0.09842089]\n"," [0.09871694 0.11431629 0.09847185 0.09854462 0.09752847 0.09854525\n","  0.09847542 0.09846945 0.09851081 0.09842089]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.633483191265469\n","[[0.09874098 0.11388114 0.09851838 0.09858111 0.09763497 0.09859174\n","  0.09851962 0.09851924 0.09855218 0.09846064]\n"," [0.09874098 0.11388114 0.09851838 0.09858111 0.09763497 0.09859174\n","  0.09851962 0.09851924 0.09855218 0.09846064]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.6348661925859505\n","[[0.09867455 0.11435382 0.09845773 0.09853148 0.09763331 0.09853449\n","  0.09846157 0.09845704 0.09849287 0.09840314]\n"," [0.09867455 0.11435382 0.09845773 0.09853148 0.09763331 0.09853449\n","  0.09846157 0.09845704 0.09849287 0.09840314]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.485837051487891\n","[[0.09861875 0.11489717 0.09840539 0.09848307 0.09749812 0.09848399\n","  0.09841134 0.09840284 0.0984453  0.09835402]\n"," [0.09861875 0.11489717 0.09840539 0.09848307 0.09749812 0.09848399\n","  0.09841134 0.09840284 0.0984453  0.09835402]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.49163986585203\n","[[0.09856782 0.11542661 0.09835749 0.09843806 0.09734406 0.09843712\n","  0.09836506 0.09835288 0.09840196 0.09830894]\n"," [0.09856782 0.11542661 0.09835749 0.09843806 0.09734406 0.09843712\n","  0.09836506 0.09835288 0.09840196 0.09830894]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.488623949492333\n","Epoch 23: train_loss = 2.273 | train_acc = 0.300 \n","[1 2 4 8 9 0 3 7 5 6]\n","[[0.09863476 0.11509289 0.0984087  0.09848052 0.09727181 0.09847983\n","  0.09841267 0.09840645 0.09845197 0.0983604 ]\n"," [0.09863476 0.11509289 0.0984087  0.09848052 0.09727181 0.09847983\n","  0.09841267 0.09840645 0.09845197 0.0983604 ]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.646577617744176\n","[[0.09857625 0.11553468 0.09835287 0.09842641 0.09727939 0.09842596\n","  0.09835948 0.09834914 0.09839583 0.09829999]\n"," [0.09857625 0.11553468 0.09835287 0.09842641 0.09727939 0.09842596\n","  0.09835948 0.09834914 0.09839583 0.09829999]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.477915917210925\n","[[0.09860664 0.11520489 0.09840612 0.0984712  0.09721685 0.09847828\n","  0.09840926 0.09840514 0.09844869 0.09835293]\n"," [0.09860664 0.11520489 0.09840612 0.0984712  0.09721685 0.09847828\n","  0.09840926 0.09840514 0.09844869 0.09835293]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.648730457719058\n","[[0.09854481 0.11564117 0.09834898 0.09842432 0.09722487 0.09842422\n","  0.09835429 0.09834644 0.09839173 0.09829917]\n"," [0.09854481 0.11564117 0.09834898 0.09842432 0.09722487 0.09842422\n","  0.09835429 0.09834644 0.09839173 0.09829917]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.475730589876114\n","[[0.0984796  0.11612266 0.09828381 0.09836505 0.09723779 0.09836462\n","  0.09829425 0.09828235 0.09832964 0.09824022]\n"," [0.0984796  0.11612266 0.09828381 0.09836505 0.09723779 0.09836462\n","  0.09829425 0.09828235 0.09832964 0.09824022]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.473004176502717\n","Epoch 24: train_loss = 2.272 | train_acc = 0.300 \n","[7 9 1 3 2 0 4 8 5 6]\n","[[0.09850957 0.11580432 0.09833512 0.09841721 0.09717526 0.09840805\n","  0.09834202 0.09833642 0.09838077 0.09829127]\n"," [0.09850957 0.11580432 0.09833512 0.09841721 0.09717526 0.09840805\n","  0.09834202 0.09833642 0.09838077 0.09829127]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.649778757022088\n","[[0.09848268 0.11621804 0.0982783  0.09836218 0.09718141 0.09835302\n","  0.09828682 0.09827734 0.09832295 0.09823725]\n"," [0.09848268 0.11621804 0.0982783  0.09836218 0.09718141 0.09835302\n","  0.09828682 0.09827734 0.09832295 0.09823725]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.470161793277898\n","[[0.09851485 0.11590242 0.09832982 0.09840461 0.09711521 0.09840355\n","  0.09833506 0.09833158 0.09837404 0.09828886]\n"," [0.09851485 0.11590242 0.09832982 0.09840461 0.09711521 0.09840355\n","  0.09833506 0.09833158 0.09837404 0.09828886]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.650535640099875\n","[[0.09845908 0.11631637 0.09827672 0.09835271 0.09712971 0.0983526\n","  0.09828436 0.09827672 0.09832011 0.09823162]\n"," [0.09845908 0.11631637 0.09827672 0.09835271 0.09712971 0.0983526\n","  0.09828436 0.09827672 0.09832011 0.09823162]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.471868614398508\n","[[0.09839781 0.11676925 0.09821458 0.09829573 0.09714741 0.09829598\n","  0.09822738 0.09821585 0.09826098 0.09817502]\n"," [0.09839781 0.11676925 0.09821458 0.09829573 0.09714741 0.09829598\n","  0.09822738 0.09821585 0.09826098 0.09817502]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.46815612663874\n","Epoch 25: train_loss = 2.271 | train_acc = 0.300 \n","[0 2 1 7 8 9 4 6 3 5]\n","[[0.09842752 0.11646721 0.09826416 0.09833677 0.09708212 0.0983451\n","  0.09827371 0.09826823 0.09831036 0.09822483]\n"," [0.09842752 0.11646721 0.09826416 0.09833677 0.09708212 0.0983451\n","  0.09827371 0.09826823 0.09831036 0.09822483]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.651470586351979\n","[[0.09847352 0.11607257 0.09830195 0.09837297 0.09718052 0.09837634\n","  0.09830826 0.09830831 0.09834283 0.09826274]\n"," [0.09847352 0.11607257 0.09830195 0.09837297 0.09718052 0.09837634\n","  0.09830826 0.09830831 0.09834283 0.09826274]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.63695681910179\n","[[0.09843813 0.11650126 0.09826673 0.09834053 0.09702702 0.09834105\n","  0.09827398 0.09827065 0.09831137 0.09822928]\n"," [0.09843813 0.11650126 0.09826673 0.09834053 0.09702702 0.09834105\n","  0.09827398 0.09827065 0.09831137 0.09822928]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.482618962940609\n","[[0.09838022 0.11694747 0.09820969 0.09828524 0.09703891 0.09828586\n","  0.09821939 0.09821148 0.09825356 0.09816816]\n"," [0.09838022 0.11694747 0.09820969 0.09828524 0.09703891 0.09828586\n","  0.09821939 0.09821148 0.09825356 0.09816816]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.46710379585673\n","[[0.09832251 0.11737012 0.09815069 0.09823142 0.09706133 0.09823239\n","  0.09816544 0.09815396 0.09819764 0.0981145 ]\n"," [0.09832251 0.11737012 0.09815069 0.09823142 0.09706133 0.09823239\n","  0.09816544 0.09815396 0.09819764 0.0981145 ]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.463674226290824\n","Epoch 26: train_loss = 2.270 | train_acc = 0.300 \n","[4 8 1 0 5 2 6 3 7 9]\n","[[0.09826914 0.11777141 0.09809848 0.09818018 0.09708283 0.09818115\n","  0.09811517 0.09809988 0.0981441  0.09805766]\n"," [0.09826914 0.11777141 0.09809848 0.09818018 0.09708283 0.09818115\n","  0.09811517 0.09809988 0.0981441  0.09805766]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.461209395874405\n","[[0.09830573 0.11747526 0.09812455 0.09819741 0.09717344 0.09820822\n","  0.09813815 0.09812805 0.09816544 0.09808376]\n"," [0.09830573 0.11747526 0.09812455 0.09819741 0.09717344 0.09820822\n","  0.09813815 0.09812805 0.09816544 0.09808376]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.640338282343864\n","[[0.09833929 0.11714031 0.09817415 0.09824218 0.09712371 0.09825394\n","  0.09818744 0.09818394 0.09821853 0.0981365 ]\n"," [0.09833929 0.11714031 0.09817415 0.09824218 0.09712371 0.09825394\n","  0.09818744 0.09818394 0.09821853 0.0981365 ]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.652782147964757\n","[[0.09822814 0.11817471 0.09804817 0.09812692 0.09705786 0.09813585\n","  0.09806673 0.09805142 0.09809492 0.09801529]\n"," [0.09822814 0.11817471 0.09804817 0.09812692 0.09705786 0.09813585\n","  0.09806673 0.09805142 0.09809492 0.09801529]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.271182289513047\n","[[0.09824761 0.11796659 0.09808648 0.09816602 0.09698695 0.09816696\n","  0.09810141 0.09809164 0.0981336  0.09805273]\n"," [0.09824761 0.11796659 0.09808648 0.09816602 0.09698695 0.09816696\n","  0.09810141 0.09809164 0.0981336  0.09805273]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.654273915043431\n","Epoch 27: train_loss = 2.268 | train_acc = 0.300 \n","[6 0 3 2 4 1 8 5 9 7]\n","[[0.0982017  0.11828839 0.09804352 0.09812343 0.09700419 0.09813134\n","  0.09805933 0.09804681 0.09808905 0.09801222]\n"," [0.0982017  0.11828839 0.09804352 0.09812343 0.09700419 0.09813134\n","  0.09805933 0.09804681 0.09808905 0.09801222]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.4560781061050365\n","[[0.09817117 0.11865876 0.09801458 0.09809533 0.09686447 0.09810181\n","  0.09803055 0.09801589 0.09806287 0.09798457]\n"," [0.09817117 0.11865876 0.09801458 0.09809533 0.09686447 0.09810181\n","  0.09803055 0.09801589 0.09806287 0.09798457]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.465945919070752\n","[[0.09820554 0.1183978  0.09803869 0.09811119 0.09694893 0.09811945\n","  0.09805212 0.09804211 0.09808235 0.09800183]\n"," [0.09820554 0.1183978  0.09803869 0.09811119 0.09694893 0.09811945\n","  0.09805212 0.09804211 0.09808235 0.09800183]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.643461825602891\n","[[0.09816277 0.11869645 0.09799436 0.09807095 0.0969808  0.09807901\n","  0.09801236 0.09799993 0.0980404  0.09796296]\n"," [0.09816277 0.11869645 0.09799436 0.09807095 0.0969808  0.09807901\n","  0.09801236 0.09799993 0.0980404  0.09796296]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.4540312546912535\n","[[0.09818574 0.11845982 0.09803603 0.098113   0.09691075 0.09811362\n","  0.09805056 0.09804387 0.09808273 0.09800388]\n"," [0.09818574 0.11845982 0.09803603 0.098113   0.09691075 0.09811362\n","  0.09805056 0.09804387 0.09808273 0.09800388]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.655600206776453\n","Epoch 28: train_loss = 2.268 | train_acc = 0.300 \n","[6 8 9 2 3 7 4 0 5 1]\n","[[0.09809401 0.11930838 0.09793153 0.09801658 0.09686987 0.09801452\n","  0.09795007 0.09793325 0.09797831 0.0979035 ]\n"," [0.09809401 0.11930838 0.09793153 0.09801658 0.09686987 0.09801452\n","  0.09795007 0.09793325 0.09797831 0.0979035 ]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.252087424875289\n","[[0.09813434 0.11916766 0.0979877  0.0980665  0.09659027 0.09806227\n","  0.09800197 0.09799124 0.09804025 0.09795779]\n"," [0.09813434 0.11916766 0.0979877  0.0980665  0.09659027 0.09806227\n","  0.09800197 0.09799124 0.09804025 0.09795779]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.674554602450883\n","[[0.09809506 0.11942538 0.0979523  0.09803852 0.09661387 0.09802685\n","  0.09796697 0.09795435 0.09800277 0.09792394]\n"," [0.09809506 0.11942538 0.0979523  0.09803852 0.09661387 0.09802685\n","  0.09796697 0.09795435 0.09800277 0.09792394]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.447458353214146\n","[[0.09809811 0.11919545 0.09797179 0.09805016 0.09672174 0.09804726\n","  0.09798454 0.09797618 0.09801747 0.09793731]\n"," [0.09809811 0.11919545 0.09797179 0.09805016 0.09672174 0.09804726\n","  0.09798454 0.09797618 0.09801747 0.09793731]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.645733390167875\n","[[0.09812837 0.11891226 0.09799245 0.09806636 0.0968264  0.09806697\n","  0.098006   0.09800265 0.0980376  0.09796094]\n"," [0.09812837 0.11891226 0.09799245 0.09806636 0.0968264  0.09806697\n","  0.098006   0.09800265 0.0980376  0.09796094]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.644343610869107\n","Epoch 29: train_loss = 2.266 | train_acc = 0.300 \n","[2 3 7 4 1 9 8 6 0 5]\n","[[0.09810905 0.11920716 0.09797345 0.09804783 0.09668222 0.09804669\n","  0.09798707 0.09798183 0.09802142 0.09794327]\n"," [0.09810905 0.11920716 0.09797345 0.09804783 0.09668222 0.09804669\n","  0.09798707 0.09798183 0.09802142 0.09794327]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.463218206898578\n","[[0.09811499 0.11896948 0.09799585 0.09806988 0.09677342 0.098064\n","  0.09800742 0.09800621 0.09803972 0.09795903]\n"," [0.09811499 0.11896948 0.09799585 0.09806988 0.09677342 0.098064\n","  0.09800742 0.09800621 0.09803972 0.09795903]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.645280937387429\n","[[0.09816711 0.11870096 0.09804195 0.09810919 0.09668418 0.09810316\n","  0.09804974 0.09805383 0.09808629 0.0980036 ]\n"," [0.09816711 0.11870096 0.09804195 0.09810919 0.09668418 0.09810316\n","  0.09804974 0.09805383 0.09808629 0.0980036 ]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.657389483980204\n","[[0.09807952 0.11951515 0.09793955 0.09801436 0.09665841 0.09800692\n","  0.09795165 0.09794518 0.09798354 0.09790572]\n"," [0.09807952 0.11951515 0.09793955 0.09801436 0.09665841 0.09800692\n","  0.09795165 0.09794518 0.09798354 0.09790572]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.248624304773809\n","[[0.09807812 0.11932852 0.09795046 0.09802205 0.09675662 0.09802312\n","  0.09796416 0.09796153 0.09799467 0.09792074]\n"," [0.09807812 0.11932852 0.09795046 0.09802205 0.09675662 0.09802312\n","  0.09796416 0.09796153 0.09799467 0.09792074]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.6458453205181485\n","Epoch 30: train_loss = 2.266 | train_acc = 0.300 \n","[2 6 5 3 1 0 7 4 9 8]\n","[[0.09806085 0.1196001  0.09793453 0.09800635 0.09661233 0.09800622\n","  0.09794823 0.09794385 0.09798139 0.09790615]\n"," [0.09806085 0.1196001  0.09793453 0.09800635 0.09661233 0.09800622\n","  0.09794823 0.09794385 0.09798139 0.09790615]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.460650547356636\n","[[0.09802204 0.11986517 0.09789382 0.09796974 0.0966476  0.0979701\n","  0.09791209 0.09790539 0.09794309 0.09787098]\n"," [0.09802204 0.11986517 0.09789382 0.09796974 0.0966476  0.0979701\n","  0.09791209 0.09790539 0.09794309 0.09787098]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.445259667086861\n","[[0.09804458 0.11965416 0.09790859 0.09797743 0.09674354 0.0979862\n","  0.09792448 0.09792159 0.09795363 0.09788581]\n"," [0.09804458 0.11965416 0.09790859 0.09797743 0.09674354 0.0979862\n","  0.09792448 0.09792159 0.09795363 0.09788581]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.645261609425347\n","[[0.09805228 0.11939969 0.09793205 0.09799944 0.09684288 0.09800482\n","  0.09794571 0.09794688 0.09797298 0.09790326]\n"," [0.09805228 0.11939969 0.09793205 0.09799944 0.09684288 0.09800482\n","  0.09794571 0.09794688 0.09797298 0.09790326]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.646568886984211\n","[[0.09803485 0.11966456 0.09791603 0.09798426 0.09670909 0.09798686\n","  0.09792922 0.09792838 0.09795927 0.09788747]\n"," [0.09803485 0.11966456 0.09791603 0.09798426 0.09670909 0.09798686\n","  0.09792922 0.09792838 0.09795927 0.09788747]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.459110650961776\n","Epoch 31: train_loss = 2.266 | train_acc = 0.300 \n","[0 3 6 5 4 7 9 8 1 2]\n","[[0.09799689 0.11992393 0.09787963 0.09794757 0.09673849 0.09795646\n","  0.09789338 0.0978898  0.09792096 0.09785289]\n"," [0.09799689 0.11992393 0.09787963 0.09794757 0.09673849 0.09795646\n","  0.09789338 0.0978898  0.09792096 0.09785289]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.444129801578811\n","[[0.09796065 0.12016406 0.09784147 0.09791311 0.09677988 0.0979226\n","  0.09785951 0.0978537  0.09788493 0.0978201 ]\n"," [0.09796065 0.12016406 0.09784147 0.09791311 0.09677988 0.0979226\n","  0.09785951 0.0978537  0.09788493 0.0978201 ]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.443304055250126\n","[[0.09796299 0.11997458 0.09785794 0.09792952 0.09686563 0.09793426\n","  0.09787418 0.0978719  0.09789815 0.09783085]\n"," [0.09796299 0.11997458 0.09785794 0.09792952 0.09686563 0.09793426\n","  0.09787418 0.0978719  0.09789815 0.09783085]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.648022532655894\n","[[0.09794888 0.1202018  0.09784517 0.0979168  0.0967454  0.09791898\n","  0.09786062 0.0978569  0.09788724 0.0978182 ]\n"," [0.09794888 0.1202018  0.09784517 0.0979168  0.0967454  0.09791898\n","  0.09786062 0.0978569  0.09788724 0.0978182 ]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.454255725646167\n","[[0.09799011 0.12003732 0.09787946 0.09794512 0.09664283 0.09794712\n","  0.09789164 0.09789223 0.09792272 0.09785147]\n"," [0.09799011 0.12003732 0.09787946 0.09794512 0.09664283 0.09794712\n","  0.09789164 0.09789223 0.09792272 0.09785147]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.659622018917348\n","Epoch 32: train_loss = 2.265 | train_acc = 0.300 \n","[0 1 4 6 7 3 2 8 9 5]\n","[[0.09801649 0.11980319 0.09789872 0.09795737 0.09672723 0.097967\n","  0.09790845 0.09791274 0.09793811 0.09787072]\n"," [0.09801649 0.11980319 0.09789872 0.09795737 0.09672723 0.097967\n","  0.09790845 0.09791274 0.09793811 0.09787072]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.645744166672854\n","[[0.09798378 0.12002062 0.09786673 0.09792506 0.09677372 0.0979352\n","  0.09787748 0.09787879 0.09790362 0.09783499]\n"," [0.09798378 0.12002062 0.09786673 0.09792506 0.09677372 0.0979352\n","  0.09787748 0.09787879 0.09790362 0.09783499]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.444564716813506\n","[[0.09794735 0.12025253 0.09783274 0.09789704 0.0968118  0.09790139\n","  0.09784405 0.09784286 0.09786809 0.09780216]\n"," [0.09794735 0.12025253 0.09783274 0.09789704 0.0968118  0.09790139\n","  0.09784405 0.09784286 0.09786809 0.09780216]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.44200028832403\n","[[0.09793182 0.12048997 0.09781861 0.09788342 0.09668895 0.09788534\n","  0.09782971 0.09782682 0.09785612 0.09778925]\n"," [0.09793182 0.12048997 0.09781861 0.09788342 0.09668895 0.09788534\n","  0.09782971 0.09782682 0.09785612 0.09778925]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.452444967194838\n","[[0.09795322 0.12034407 0.09785022 0.09791332 0.09658309 0.09791492\n","  0.09786159 0.09786317 0.09789346 0.09782294]\n"," [0.09795322 0.12034407 0.09785022 0.09791332 0.09658309 0.09791492\n","  0.09786159 0.09786317 0.09789346 0.09782294]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.661668968826168\n","Epoch 33: train_loss = 2.265 | train_acc = 0.300 \n","[0 8 9 1 5 3 4 2 7 6]\n","[[0.09792323 0.12052684 0.09782215 0.09788433 0.09662459 0.09789144\n","  0.0978341  0.09783346 0.09786292 0.09779696]\n"," [0.09792323 0.12052684 0.09782215 0.09788433 0.09662459 0.09789144\n","  0.0978341  0.09783346 0.09786292 0.09779696]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.439779019072658\n","[[0.09796231 0.12037876 0.09785395 0.09791113 0.09652346 0.09791816\n","  0.09786273 0.09786606 0.09789611 0.09782732]\n"," [0.09796231 0.12037876 0.09785395 0.09791113 0.09652346 0.09791816\n","  0.09786273 0.09786606 0.09789611 0.09782732]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.6611416848547265\n","[[0.09793287 0.12056012 0.09782325 0.0978836  0.09656832 0.09789086\n","  0.09783573 0.09783718 0.09786691 0.09780116]\n"," [0.09793287 0.12056012 0.09782325 0.0978836  0.09656832 0.09789086\n","  0.09783573 0.09783718 0.09786691 0.09780116]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.440199701025791\n","[[0.09795506 0.12041778 0.0978574  0.09791275 0.09646555 0.09791942\n","  0.0978677  0.0978727  0.09790245 0.0978292 ]\n"," [0.09795506 0.12041778 0.0978574  0.09791275 0.09646555 0.09791942\n","  0.0978677  0.0978727  0.09790245 0.0978292 ]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.663101527228829\n","[[0.09792456 0.12059606 0.09782927 0.09788894 0.09651326 0.09789123\n","  0.09784    0.09784278 0.09787171 0.09780218]\n"," [0.09792456 0.12059606 0.09782927 0.09788894 0.09651326 0.09789123\n","  0.09784    0.09784278 0.09787171 0.09780218]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.439230415030613\n","Epoch 34: train_loss = 2.264 | train_acc = 0.300 \n","[7 8 9 6 4 1 3 0 2 5]\n","[[0.09789192 0.12078457 0.09779847 0.09786369 0.09657013 0.09786041\n","  0.09780965 0.09781004 0.09783808 0.09777303]\n"," [0.09789192 0.12078457 0.09779847 0.09786369 0.09657013 0.09786041\n","  0.09780965 0.09781004 0.09783808 0.09777303]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.437926367877607\n","[[0.09788349 0.12098691 0.09779122 0.0978566  0.09642857 0.09785115\n","  0.09780168 0.09780097 0.09783328 0.09776612]\n"," [0.09788349 0.12098691 0.09779122 0.0978566  0.09642857 0.09785115\n","  0.09780168 0.09780097 0.09783328 0.09776612]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.4510256392807275\n","[[0.09790031 0.12083954 0.09780075 0.09786039 0.09651112 0.09785739\n","  0.09780966 0.09781151 0.0978392  0.09777013]\n"," [0.09790031 0.12083954 0.09780075 0.09786039 0.09651112 0.09785739\n","  0.09780966 0.09781151 0.0978392  0.09777013]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.648941729904788\n","[[0.09787234 0.12098717 0.09777499 0.0978333  0.09657133 0.09783578\n","  0.09778424 0.09778436 0.0978108  0.09774569]\n"," [0.09787234 0.12098717 0.09777499 0.0978333  0.09657133 0.09783578\n","  0.09778424 0.09778436 0.0978108  0.09774569]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.436535682434498\n","[[0.09789104 0.12086817 0.09780229 0.09785931 0.09647019 0.09786209\n","  0.09781209 0.09781603 0.09784354 0.09777525]\n"," [0.09789104 0.12086817 0.09780229 0.09785931 0.09647019 0.09786209\n","  0.09781209 0.09781603 0.09784354 0.09777525]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.663328544375797\n","Epoch 35: train_loss = 2.264 | train_acc = 0.300 \n","[1 3 8 5 4 2 9 7 6 0]\n","[[0.09788171 0.12101421 0.09777662 0.09783299 0.09651652 0.09783633\n","  0.09778655 0.09778871 0.09781547 0.09775088]\n"," [0.09788171 0.12101421 0.09777662 0.09783299 0.09651652 0.09783633\n","  0.09778655 0.09778871 0.09781547 0.09775088]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.435842925796708\n","[[0.09785375 0.12117074 0.09774753 0.09780661 0.09657462 0.09781045\n","  0.09776103 0.09776143 0.09778737 0.09772648]\n"," [0.09785375 0.12117074 0.09774753 0.09780661 0.09657462 0.09781045\n","  0.09776103 0.09776143 0.09778737 0.09772648]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.435922083437244\n","[[0.09787171 0.12106379 0.09777641 0.09783105 0.09647583 0.09783451\n","  0.09778789 0.09779144 0.09781786 0.09774951]\n"," [0.09787171 0.12106379 0.09777641 0.09783105 0.09647583 0.09783451\n","  0.09778789 0.09779144 0.09781786 0.09774951]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.663809837178155\n","[[0.09789484 0.12092795 0.09781176 0.09786646 0.09634745 0.09786486\n","  0.09782025 0.0978277  0.09785568 0.09778306]\n"," [0.09789484 0.12092795 0.09781176 0.09786646 0.09634745 0.09786486\n","  0.09782025 0.0978277  0.09785568 0.09778306]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.6639457447521835\n","[[0.09786815 0.121072   0.09778704 0.09784104 0.09640313 0.09784336\n","  0.09779594 0.09780124 0.09782785 0.09776025]\n"," [0.09786815 0.121072   0.09778704 0.09784104 0.09640313 0.09784336\n","  0.09779594 0.09780124 0.09782785 0.09776025]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.435757318242546\n","Epoch 36: train_loss = 2.264 | train_acc = 0.300 \n","[4 1 6 2 5 8 7 0 9 3]\n","[[0.09788175 0.12092713 0.09779423 0.09784323 0.09650147 0.09784815\n","  0.09780176 0.09780911 0.09783116 0.097762  ]\n"," [0.09788175 0.12092713 0.09779423 0.09784323 0.09650147 0.09784815\n","  0.09780176 0.09780911 0.09783116 0.097762  ]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.64921451260058\n","[[0.09787785 0.12109814 0.09779097 0.09784005 0.09635813 0.09784301\n","  0.09779805 0.09780419 0.0978304  0.09775921]\n"," [0.09787785 0.12109814 0.09779097 0.09784005 0.09635813 0.09784301\n","  0.09779805 0.09780419 0.0978304  0.09775921]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.45083751542774\n","[[0.09785083 0.12125042 0.09776308 0.09781499 0.09641161 0.09781843\n","  0.09777356 0.09777776 0.09780326 0.09773606]\n"," [0.09785083 0.12125042 0.09776308 0.09781499 0.09641161 0.09781843\n","  0.09777356 0.09777776 0.09780326 0.09773606]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.435105571255972\n","[[0.09784588 0.1211168  0.09776947 0.09782094 0.09650715 0.09782621\n","  0.0977787  0.0977852  0.09780658 0.09774307]\n"," [0.09784588 0.1211168  0.09776947 0.09782094 0.09650715 0.09782621\n","  0.0977787  0.0977852  0.09780658 0.09774307]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.649179394924648\n","[[0.09784257 0.12127357 0.09776777 0.09781955 0.09636698 0.09782287\n","  0.09777603 0.09778172 0.09780752 0.09774142]\n"," [0.09784257 0.12127357 0.09776777 0.09781955 0.09636698 0.09782287\n","  0.09777603 0.09778172 0.09780752 0.09774142]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.4492980062341925\n","Epoch 37: train_loss = 2.263 | train_acc = 0.300 \n","[6 4 3 0 1 2 5 9 7 8]\n","[[0.09781853 0.12141313 0.09774479 0.09779556 0.09642122 0.09779942\n","  0.09775372 0.09775725 0.09778154 0.09771482]\n"," [0.09781853 0.12141313 0.09774479 0.09779556 0.09642122 0.09779942\n","  0.09775372 0.09775725 0.09778154 0.09771482]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.434258294339503\n","[[0.09779301 0.12153729 0.09772142 0.09777109 0.09648618 0.09777979\n","  0.09773056 0.09773251 0.09775544 0.0976927 ]\n"," [0.09779301 0.12153729 0.09772142 0.09777109 0.09648618 0.09777979\n","  0.09773056 0.09773251 0.09775544 0.0976927 ]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.432571523585909\n","[[0.09782478 0.12145031 0.09774626 0.09779251 0.09637527 0.09780084\n","  0.09775292 0.09775793 0.09778247 0.09771672]\n"," [0.09782478 0.12145031 0.09774626 0.09779251 0.09637527 0.09780084\n","  0.09775292 0.09775793 0.09778247 0.09771672]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.664083030775961\n","[[0.09784834 0.12134394 0.0977766  0.0978225  0.09623618 0.09782951\n","  0.09778337 0.097792   0.09781898 0.09774859]\n"," [0.09784834 0.12134394 0.0977766  0.0978225  0.09623618 0.09782951\n","  0.09778337 0.097792   0.09781898 0.09774859]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.666019927315501\n","[[0.09782423 0.12145799 0.09775465 0.09780351 0.09629882 0.0978075\n","  0.09776196 0.09776877 0.09779414 0.09772845]\n"," [0.09782423 0.12145799 0.09775465 0.09780351 0.09629882 0.0978075\n","  0.09776196 0.09776877 0.09779414 0.09772845]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.432981716599735\n","Epoch 38: train_loss = 2.263 | train_acc = 0.300 \n","[7 3 5 6 8 2 9 1 4 0]\n","[[0.09779783 0.12157816 0.09773063 0.09778334 0.09637184 0.09778365\n","  0.09773823 0.09774337 0.09776724 0.09770573]\n"," [0.09779783 0.12157816 0.09773063 0.09778334 0.09637184 0.09778365\n","  0.09773823 0.09774337 0.09776724 0.09770573]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.432198998694437\n","[[0.09777416 0.12168914 0.09770617 0.09776124 0.09644087 0.09776213\n","  0.0977169  0.0977207  0.09774327 0.09768542]\n"," [0.09777416 0.12168914 0.09770617 0.09776124 0.09644087 0.09776213\n","  0.0977169  0.0977207  0.09774327 0.09768542]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.432076139658761\n","[[0.09777435 0.12181857 0.09770746 0.09776174 0.09630465 0.09776101\n","  0.09771749 0.09772074 0.09774686 0.09768714]\n"," [0.09777435 0.12181857 0.09770746 0.09776174 0.09630465 0.09776101\n","  0.09771749 0.09772074 0.09774686 0.09768714]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.445461168164559\n","[[0.09780811 0.12175384 0.09773466 0.09778578 0.09615408 0.09778406\n","  0.09774184 0.0977481  0.09777704 0.09771249]\n"," [0.09780811 0.12175384 0.09773466 0.09778578 0.09615408 0.09778406\n","  0.09774184 0.0977481  0.09777704 0.09771249]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.666551175280369\n","[[0.09780511 0.12163683 0.0977404  0.0977868  0.09624636 0.09779026\n","  0.09774674 0.0977546  0.09777899 0.09771391]\n"," [0.09780511 0.12163683 0.0977404  0.0977868  0.09624636 0.09779026\n","  0.09774674 0.0977546  0.09777899 0.09771391]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.650641675694725\n","Epoch 39: train_loss = 2.263 | train_acc = 0.300 \n","[9 7 1 4 5 0 8 2 3 6]\n","[[0.09782646 0.12154198 0.0977709  0.09781786 0.09611002 0.09781709\n","  0.09777479 0.09778577 0.0978124  0.09774272]\n"," [0.09782646 0.12154198 0.0977709  0.09781786 0.09611002 0.09781709\n","  0.09777479 0.09778577 0.0978124  0.09774272]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.666909796930174\n","[[0.0978373  0.12140566 0.09777724 0.09782015 0.09620659 0.09782134\n","  0.09778012 0.09779272 0.09781495 0.09774394]\n"," [0.0978373  0.12140566 0.09777724 0.09782015 0.09620659 0.09782134\n","  0.09778012 0.09779272 0.09781495 0.09774394]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.649853432594963\n","[[0.09783452 0.12125857 0.09778165 0.09782279 0.0963176  0.09782919\n","  0.09778585 0.0978004  0.09781849 0.09775092]\n"," [0.09783452 0.12125857 0.09778165 0.09782279 0.0963176  0.09782919\n","  0.09778585 0.0978004  0.09781849 0.09775092]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.649550555817759\n","[[0.09783581 0.12140988 0.09778246 0.09782404 0.09615874 0.09782849\n","  0.09778632 0.09779944 0.09782213 0.09775268]\n"," [0.09783581 0.12140988 0.09778246 0.09782404 0.09615874 0.09782849\n","  0.09778632 0.09779944 0.09782213 0.09775268]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.450337899317806\n","[[0.09779268 0.12175675 0.09773279 0.09777707 0.09619938 0.0977811\n","  0.09773864 0.09774609 0.09776968 0.09770584]\n"," [0.09779268 0.12175675 0.09773279 0.09777707 0.09619938 0.0977811\n","  0.09773864 0.09774609 0.09776968 0.09770584]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.211460184224453\n","Epoch 40: train_loss = 2.263 | train_acc = 0.300 \n","[3 1 8 0 6 2 5 9 7 4]\n","[[0.09778124 0.12186445 0.09770845 0.0977521  0.09627714 0.09775709\n","  0.09771441 0.09772021 0.09774227 0.09768263]\n"," [0.09778124 0.12186445 0.09770845 0.0977521  0.09627714 0.09775709\n","  0.09771441 0.09772021 0.09774227 0.09768263]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.4298684040600715\n","[[0.09775917 0.12195035 0.09768855 0.09773086 0.09635338 0.09774009\n","  0.09769482 0.09769914 0.09771918 0.09766444]\n"," [0.09775917 0.12195035 0.09768855 0.09773086 0.09635338 0.09774009\n","  0.09769482 0.09769914 0.09771918 0.09766444]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.429584679187642\n","[[0.09776139 0.12205963 0.0976929  0.0977348  0.09621247 0.09774243\n","  0.09769841 0.09770258 0.09772629 0.09766911]\n"," [0.09776139 0.12205963 0.0976929  0.0977348  0.09621247 0.09774243\n","  0.09769841 0.09770258 0.09772629 0.09766911]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.444441845439568\n","[[0.09778098 0.12201387 0.09771787 0.09776045 0.09605312 0.09776634\n","  0.09772359 0.09773073 0.097758   0.09769505]\n"," [0.09778098 0.12201387 0.09771787 0.09776045 0.09605312 0.09776634\n","  0.09772359 0.09773073 0.097758   0.09769505]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.6685246971316445\n","[[0.09777528 0.12191557 0.09772059 0.0977627  0.09614703 0.09776747\n","  0.09772583 0.09773441 0.09775752 0.09769359]\n"," [0.09777528 0.12191557 0.09772059 0.0977627  0.09614703 0.09776747\n","  0.09772583 0.09773441 0.09775752 0.09769359]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.651131435025957\n","Epoch 41: train_loss = 2.262 | train_acc = 0.300 \n","[0 7 4 8 6 5 2 1 9 3]\n","[[0.09776813 0.12180695 0.09772225 0.0977642  0.0962533  0.09777062\n","  0.09772658 0.09773676 0.09775587 0.09769534]\n"," [0.09776813 0.12180695 0.09772225 0.0977642  0.0962533  0.09777062\n","  0.09772658 0.09773676 0.09775587 0.09769534]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.650327993537973\n","[[0.09774844 0.12188628 0.09770342 0.09774437 0.09633361 0.09775157\n","  0.09770843 0.0977167  0.09773356 0.09767363]\n"," [0.09774844 0.12188628 0.09770342 0.09774437 0.09633361 0.09775157\n","  0.09770843 0.0977167  0.09773356 0.09767363]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.430790491933115\n","[[0.09772808 0.12197119 0.09768221 0.09772485 0.09640603 0.09773273\n","  0.09768978 0.09769682 0.09771252 0.09765577]\n"," [0.09772808 0.12197119 0.09768221 0.09772485 0.09640603 0.09773273\n","  0.09768978 0.09769682 0.09771252 0.09765577]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.430006165989197\n","[[0.09775574 0.1219134  0.09770419 0.09774447 0.0962878  0.09775135\n","  0.09770965 0.09771912 0.09773737 0.09767692]\n"," [0.09775574 0.1219134  0.09770419 0.09774447 0.0962878  0.09775135\n","  0.09770965 0.09771912 0.09773737 0.09767692]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.665697064339272\n","[[0.09776304 0.12201856 0.09771249 0.0977527  0.09611893 0.09775719\n","  0.09771677 0.09772609 0.09774915 0.09768507]\n"," [0.09776304 0.12201856 0.09771249 0.0977527  0.09611893 0.09775719\n","  0.09771677 0.09772609 0.09774915 0.09768507]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.445751076614277\n","Epoch 42: train_loss = 2.262 | train_acc = 0.300 \n","[6 3 4 9 0 8 1 5 2 7]\n","[[0.097732   0.12223297 0.09767852 0.09771953 0.0961734  0.09772415\n","  0.09768407 0.09768972 0.09771213 0.09765353]\n"," [0.097732   0.12223297 0.09767852 0.09771953 0.0961734  0.09772415\n","  0.09768407 0.09768972 0.09771213 0.09765353]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.203652955516852\n","[[0.09774948 0.12220772 0.09770162 0.09774056 0.09602934 0.09774372\n","  0.09770541 0.09771306 0.09773822 0.09767086]\n"," [0.09774948 0.12220772 0.09770162 0.09774056 0.09602934 0.09774372\n","  0.09770541 0.09771306 0.09773822 0.09767086]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.669253525684157\n","[[0.09773178 0.12225817 0.09768626 0.09772363 0.09610645 0.09773021\n","  0.0976903  0.09769683 0.0977194  0.09765697]\n"," [0.09773178 0.12225817 0.09768626 0.09772363 0.09610645 0.09773021\n","  0.0976903  0.09769683 0.0977194  0.09765697]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.427164840019718\n","[[0.09773361 0.1221858  0.09768072 0.09771804 0.09620755 0.09772677\n","  0.09768601 0.09769404 0.09771368 0.09765378]\n"," [0.09773361 0.1221858  0.09768072 0.09771804 0.09620755 0.09772677\n","  0.09768601 0.09769404 0.09771368 0.09765378]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.651560878031888\n","[[0.0977518  0.12213481 0.09770528 0.09774322 0.09606925 0.09774815\n","  0.09770882 0.09771928 0.09774163 0.09767776]\n"," [0.0977518  0.12213481 0.09770528 0.09774322 0.09606925 0.09774815\n","  0.09770882 0.09771928 0.09774163 0.09767776]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.668097375591859\n","Epoch 43: train_loss = 2.262 | train_acc = 0.300 \n","[0 5 9 7 6 4 8 2 1 3]\n","[[0.0977457  0.12204566 0.09770443 0.09774216 0.09615979 0.09775112\n","  0.09770949 0.09772138 0.0977406  0.09767968]\n"," [0.0977457  0.12204566 0.09770443 0.09774216 0.09615979 0.09775112\n","  0.09770949 0.09772138 0.0977406  0.09767968]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.651139078091754\n","[[0.09776618 0.12198322 0.09773149 0.09776977 0.09601417 0.09777512\n","  0.0977345  0.09774882 0.09777112 0.09770563]\n"," [0.09776618 0.12198322 0.09773149 0.09776977 0.09601417 0.09777512\n","  0.0977345  0.09774882 0.09777112 0.09770563]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.66839942619762\n","[[0.09774833 0.12205271 0.09771419 0.09775153 0.09609225 0.09775746\n","  0.0977179  0.09773028 0.09775005 0.09768528]\n"," [0.09774833 0.12205271 0.09771419 0.09775153 0.09609225 0.09775746\n","  0.0977179  0.09773028 0.09775005 0.09768528]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.429306638955098\n","[[0.09775675 0.12216032 0.09772229 0.09775956 0.09592129 0.09776308\n","  0.09772522 0.09773698 0.09776111 0.09769342]\n"," [0.09775675 0.12216032 0.09772229 0.09775956 0.09592129 0.09776308\n","  0.09772522 0.09773698 0.09776111 0.09769342]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.446648355731741\n","[[0.0977473  0.12222326 0.09770374 0.09774015 0.09600146 0.09774487\n","  0.09770671 0.0977172  0.09773934 0.09767598]\n"," [0.0977473  0.12222326 0.09770374 0.09774015 0.09600146 0.09774487\n","  0.09770671 0.0977172  0.09773934 0.09767598]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.427275639189309\n","Epoch 44: train_loss = 2.262 | train_acc = 0.300 \n","[2 4 1 3 5 6 0 7 9 8]\n","[[0.09776907 0.1221887  0.09773013 0.09776455 0.09583869 0.0977677\n","  0.09773158 0.0977439  0.09776893 0.09769675]\n"," [0.09776907 0.1221887  0.09773013 0.09776455 0.09583869 0.0977677\n","  0.09773158 0.0977439  0.09776893 0.09769675]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.6709758019412515\n","[[0.09776053 0.1222399  0.09771229 0.09774585 0.0959248  0.09775033\n","  0.09771383 0.09772498 0.0977477  0.0976798 ]\n"," [0.09776053 0.1222399  0.09771229 0.09774585 0.0959248  0.09775033\n","  0.09771383 0.09772498 0.0977477  0.0976798 ]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.427004180399705\n","[[0.09773953 0.12229453 0.09769134 0.09772639 0.09602667 0.0977322\n","  0.09769547 0.09770545 0.09772565 0.09766277]\n"," [0.09773953 0.12229453 0.09769134 0.09772639 0.09602667 0.0977322\n","  0.09769547 0.09770545 0.09772565 0.09766277]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.42726540322032\n","[[0.09772866 0.12222448 0.0976876  0.09772234 0.09613425 0.09773023\n","  0.0976913  0.09770223 0.09771892 0.09765998]\n"," [0.09772866 0.12222448 0.0976876  0.09772234 0.09613425 0.09773023\n","  0.0976913  0.09770223 0.09771892 0.09765998]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.65116937373948\n","[[0.09773783 0.12230633 0.09769791 0.09773274 0.09597245 0.09773826\n","  0.0977005  0.09771129 0.09773235 0.09767035]\n"," [0.09773783 0.12230633 0.09769791 0.09773274 0.09597245 0.09773826\n","  0.0977005  0.09771129 0.09773235 0.09767035]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.444920583909613\n","Epoch 45: train_loss = 2.262 | train_acc = 0.300 \n","[5 8 9 0 2 7 4 1 6 3]\n","[[0.09772029 0.12235718 0.09768023 0.09771652 0.09605216 0.09772291\n","  0.09768528 0.09769503 0.09771414 0.09765627]\n"," [0.09772029 0.12235718 0.09768023 0.09771652 0.09605216 0.09772291\n","  0.09768528 0.09769503 0.09771414 0.09765627]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.426866944307595\n","[[0.09773923 0.12233    0.09770427 0.09773899 0.09589393 0.0977459\n","  0.09770739 0.09771912 0.09774179 0.09767937]\n"," [0.09773923 0.12233    0.09770427 0.09773899 0.09589393 0.0977459\n","  0.09770739 0.09771912 0.09774179 0.09767937]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.669896620653693\n","[[0.09776411 0.12229609 0.09773432 0.09776965 0.09569347 0.09777278\n","  0.09773546 0.09774949 0.09777634 0.09770828]\n"," [0.09776411 0.12229609 0.09773432 0.09776965 0.09569347 0.09777278\n","  0.09773546 0.09774949 0.09777634 0.09770828]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.671746282129739\n","[[0.09776556 0.12221276 0.09773201 0.09776468 0.09580372 0.09776968\n","  0.09773269 0.09774738 0.09777    0.09770151]\n"," [0.09776556 0.12221276 0.09773201 0.09776468 0.09580372 0.09776968\n","  0.09773269 0.09774738 0.09777    0.09770151]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.651021188397543\n","[[0.09773495 0.12237994 0.0976986  0.09773177 0.09590198 0.09773756\n","  0.09770073 0.0977118  0.09773231 0.09767037]\n"," [0.09773495 0.12237994 0.0976986  0.09773177 0.09590198 0.09773756\n","  0.09770073 0.0977118  0.09773231 0.09767037]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.201249595612029\n","Epoch 46: train_loss = 2.262 | train_acc = 0.300 \n","[3 8 9 1 5 6 4 0 7 2]\n","[[0.09770738 0.12252024 0.0976694  0.09770259 0.0959952  0.09770903\n","  0.09767268 0.0976807  0.0976991  0.09764369]\n"," [0.09770738 0.12252024 0.0976694  0.09770259 0.0959952  0.09770903\n","  0.09767268 0.0976807  0.0976991  0.09764369]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.198958138675747\n","[[0.09773576 0.12251012 0.09769112 0.09772338 0.09582833 0.09772795\n","  0.09769219 0.09770227 0.09772496 0.0976639 ]\n"," [0.09773576 0.12251012 0.09769112 0.09772338 0.09582833 0.09772795\n","  0.09769219 0.09770227 0.09772496 0.0976639 ]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.670684635480695\n","[[0.09771921 0.12253784 0.09767536 0.0977087  0.09591762 0.09771453\n","  0.09767872 0.09768817 0.09770839 0.09765146]\n"," [0.09771921 0.12253784 0.09767536 0.0977087  0.09591762 0.09771453\n","  0.09767872 0.09768817 0.09770839 0.09765146]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.425441351475664\n","[[0.09770775 0.1224854  0.09766944 0.09770026 0.09603054 0.09771005\n","  0.09767279 0.09768258 0.0976989  0.0976423 ]\n"," [0.09770775 0.1224854  0.09766944 0.09770026 0.09603054 0.09771005\n","  0.09767279 0.09768258 0.0976989  0.0976423 ]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.65219533344978\n","[[0.09772619 0.12246046 0.0976931  0.09772459 0.09587171 0.0977312\n","  0.09769474 0.09770661 0.09772661 0.09766479]\n"," [0.09772619 0.12246046 0.0976931  0.09772459 0.09587171 0.0977312\n","  0.09769474 0.09770661 0.09772661 0.09766479]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.670346382291562\n","Epoch 47: train_loss = 2.262 | train_acc = 0.300 \n","[0 2 8 6 3 7 5 1 4 9]\n","[[0.09775058 0.12243816 0.09772185 0.09775221 0.09566926 0.09775817\n","  0.09772164 0.09773546 0.09775982 0.09769284]\n"," [0.09775058 0.12243816 0.09772185 0.09775221 0.09566926 0.09775817\n","  0.09772164 0.09773546 0.09775982 0.09769284]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.672116697625451\n","[[0.09772584 0.1225616  0.09769527 0.0977255  0.09575812 0.09773239\n","  0.09769634 0.0977071  0.09772885 0.09766899]\n"," [0.09772584 0.1225616  0.09769527 0.0977255  0.09575812 0.09773239\n","  0.09769634 0.0977071  0.09772885 0.09766899]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.198283032141993\n","[[0.09770559 0.12259277 0.0976776  0.09770875 0.09587157 0.09771531\n","  0.09767911 0.09768897 0.09770763 0.0976527 ]\n"," [0.09770559 0.12259277 0.0976776  0.09770875 0.09587157 0.09771531\n","  0.09767911 0.09768897 0.09770763 0.0976527 ]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.424651349864612\n","[[0.09770261 0.12254031 0.09766842 0.09770003 0.09598459 0.09770817\n","  0.09767105 0.09768185 0.09769778 0.0976452 ]\n"," [0.09770261 0.12254031 0.09766842 0.09770003 0.09598459 0.09770817\n","  0.09767105 0.09768185 0.09769778 0.0976452 ]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.6520040619178875\n","[[0.09772301 0.12252191 0.09769256 0.09772296 0.09582066 0.0977291\n","  0.09769366 0.09770611 0.09772574 0.0976643 ]\n"," [0.09772301 0.12252191 0.09769256 0.09772296 0.09582066 0.0977291\n","  0.09769366 0.09770611 0.09772574 0.0976643 ]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.67149615937442\n","Epoch 48: train_loss = 2.262 | train_acc = 0.300 \n","[8 0 2 4 9 1 6 3 7 5]\n","[[0.09770702 0.12254837 0.09767855 0.09770756 0.09591074 0.09771667\n","  0.09768001 0.09769128 0.09770814 0.09765167]\n"," [0.09770702 0.12254837 0.09767855 0.09770756 0.09591074 0.09771667\n","  0.09768001 0.09769128 0.09770814 0.09765167]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.424932604717089\n","[[0.09772787 0.12253748 0.09770272 0.09773058 0.09573808 0.09773797\n","  0.09770283 0.09771559 0.09773612 0.09767076]\n"," [0.09772787 0.12253748 0.09770272 0.09773058 0.09573808 0.09773797\n","  0.09770283 0.09771559 0.09773612 0.09767076]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.67229226826823\n","[[0.09776299 0.12252114 0.09773238 0.0977597  0.09551445 0.09776444\n","  0.09773016 0.0977451  0.09777114 0.0976985 ]\n"," [0.09776299 0.12252114 0.09773238 0.0977597  0.09551445 0.09776444\n","  0.09773016 0.0977451  0.09777114 0.0976985 ]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.673686931119214\n","[[0.09773815 0.12262993 0.0977064  0.09773364 0.09561328 0.09773941\n","  0.09770547 0.09771762 0.09774074 0.09767534]\n"," [0.09773815 0.12262993 0.0977064  0.09773364 0.09561328 0.09773941\n","  0.09770547 0.09771762 0.09774074 0.09767534]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.197168257588744\n","[[0.09772284 0.12258167 0.09769524 0.09772441 0.09574523 0.09773086\n","  0.09769599 0.09770879 0.09772827 0.09766672]\n"," [0.09772284 0.12258167 0.09769524 0.09772441 0.09574523 0.09773086\n","  0.09769599 0.09770879 0.09772827 0.09766672]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.651506388441189\n","Epoch 49: train_loss = 2.262 | train_acc = 0.300 \n","[6 5 2 9 0 7 1 4 8 3]\n","[[0.09770433 0.12260088 0.09767711 0.09770764 0.0958614  0.09771533\n","  0.09768027 0.09769221 0.09770871 0.09765213]\n"," [0.09770433 0.12260088 0.09767711 0.09770764 0.0958614  0.09771533\n","  0.09768027 0.09769221 0.09770871 0.09765213]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.424909165331931\n","[[0.09776406 0.12264349 0.0977386  0.09776977 0.09532758 0.09777061\n","  0.09773826 0.09775341 0.09778349 0.09771072]\n"," [0.09776406 0.12264349 0.0977386  0.09776977 0.09532758 0.09777061\n","  0.09773826 0.09775341 0.09778349 0.09771072]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.70087224013823\n","[[0.09775426 0.12258633 0.0977343  0.09776478 0.09542746 0.09776701\n","  0.09773375 0.09774934 0.09777552 0.09770724]\n"," [0.09775426 0.12258633 0.0977343  0.09776478 0.09542746 0.09776701\n","  0.09773375 0.09774934 0.09777552 0.09770724]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.650358981073927\n","[[0.09774855 0.12251878 0.09772515 0.09775341 0.09557552 0.09775831\n","  0.09772452 0.09774025 0.09776151 0.09769401]\n"," [0.09774855 0.12251878 0.09772515 0.09775341 0.09557552 0.09775831\n","  0.09772452 0.09774025 0.09776151 0.09769401]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 4.651271974876989\n","[[0.09772213 0.12262291 0.09769728 0.09772527 0.09569531 0.09773135\n","  0.09769791 0.09771064 0.09772876 0.09766844]\n"," [0.09772213 0.12262291 0.09769728 0.09772527 0.09569531 0.09773135\n","  0.09769791 0.09771064 0.09772876 0.09766844]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.197282802906886\n","Epoch 50: train_loss = 2.262 | train_acc = 0.300 \n","[6 9 2 3 5 0 7 8 4 1]\n","[[0.09772249 0.1226379  0.09769748 0.09772555 0.09567855 0.09773135\n","  0.09769806 0.09771064 0.09772923 0.09766874]\n"," [0.09772249 0.1226379  0.09769748 0.09772555 0.09567855 0.09773135\n","  0.09769806 0.09771064 0.09772923 0.09766874]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.445280328240109\n","[[0.09772437 0.12264495 0.09769942 0.0977275  0.09565605 0.09773301\n","  0.09769987 0.0977125  0.09773166 0.09767066]\n"," [0.09772437 0.12264495 0.09769942 0.0977275  0.09565605 0.09773301\n","  0.09769987 0.0977125  0.09773166 0.09767066]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.44545802641426\n","[[0.0977232  0.12264018 0.09769858 0.09772668 0.09566697 0.09773253\n","  0.09769919 0.09771187 0.09773072 0.09767008]\n"," [0.0977232  0.12264018 0.09769858 0.09772668 0.09566697 0.09773253\n","  0.09769919 0.09771187 0.09773072 0.09767008]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.651389105442695\n","[[0.09772124 0.1226416  0.09769687 0.09772501 0.09567955 0.0977309\n","  0.09769754 0.09771012 0.0977286  0.09766857]\n"," [0.09772124 0.1226416  0.09769687 0.09772501 0.09567955 0.0977309\n","  0.09769754 0.09771012 0.0977286  0.09766857]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.424086766544033\n","[[0.09772074 0.1226365  0.09769593 0.09772389 0.0956927  0.09772997\n","  0.09769658 0.09770918 0.09772726 0.09766724]\n"," [0.09772074 0.1226365  0.09769593 0.09772389 0.0956927  0.09772997\n","  0.09769658 0.09770918 0.09772726 0.09766724]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.651830568838909\n","Epoch 51: train_loss = 2.262 | train_acc = 0.300 \n","[7 2 0 5 3 9 1 6 4 8]\n","[[0.09772292 0.1226355  0.09769848 0.09772653 0.09567364 0.09773226\n","  0.09769897 0.09771175 0.09773029 0.09766967]\n"," [0.09772292 0.1226355  0.09769848 0.09772653 0.09567364 0.09773226\n","  0.09769897 0.09771175 0.09773029 0.09766967]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.672394698071414\n","[[0.0977218  0.12263016 0.09769772 0.0977258  0.09568452 0.09773184\n","  0.09769836 0.0977112  0.09772944 0.09766916]\n"," [0.0977218  0.12263016 0.09769772 0.0977258  0.09568452 0.09773184\n","  0.09769836 0.0977112  0.09772944 0.09766916]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 4.6514049465218825\n","[[0.09772345 0.12263638 0.09769945 0.09772756 0.09566449 0.09773336\n","  0.09769997 0.09771286 0.09773162 0.09767087]\n"," [0.09772345 0.12263638 0.09769945 0.09772756 0.09566449 0.09773336\n","  0.09769997 0.09771286 0.09773162 0.09767087]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.445439666699343\n","[[0.09772258 0.12263866 0.09769788 0.09772588 0.09567447 0.09773182\n","  0.0976984  0.0977112  0.09772969 0.09766942]\n"," [0.09772258 0.12263866 0.09769788 0.09772588 0.09567447 0.09773182\n","  0.0976984  0.0977112  0.09772969 0.09766942]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.424135630948536\n","[[0.09772074 0.12264077 0.09769616 0.09772401 0.09568707 0.09773011\n","  0.09769677 0.09770941 0.0977275  0.09766746]\n"," [0.09772074 0.12264077 0.09769616 0.09772401 0.09568707 0.09773011\n","  0.09769677 0.09770941 0.0977275  0.09766746]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.424682635233404\n","Epoch 52: train_loss = 2.262 | train_acc = 0.300 \n","[5 7 8 0 4 6 3 9 2 1]\n","[[0.09771928 0.12263564 0.0976951  0.09772314 0.09570017 0.09772931\n","  0.09769587 0.09770858 0.09772632 0.09766659]\n"," [0.09771928 0.12263564 0.0976951  0.09772314 0.09570017 0.09772931\n","  0.09769587 0.09770858 0.09772632 0.09766659]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] 4.651520813468621\n","[[0.0977174  0.12263713 0.09769341 0.09772133 0.09571256 0.09772779\n","  0.09769424 0.09770682 0.0977242  0.09766512]\n"," [0.0977174  0.12263713 0.09769341 0.09772133 0.09571256 0.09772779\n","  0.09769424 0.09770682 0.0977242  0.09766512]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] 4.424094727848228\n","[[0.09771556 0.12263932 0.09769171 0.09771949 0.09572485 0.09772614\n","  0.09769262 0.09770506 0.09772206 0.09766318]\n"," [0.09771556 0.12263932 0.09769171 0.09771949 0.09572485 0.09772614\n","  0.09769262 0.09770506 0.09772206 0.09766318]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.424738279792608\n","[[0.09771712 0.12264585 0.09769337 0.09772117 0.09570528 0.09772755\n","  0.09769415 0.09770663 0.09772415 0.09766475]\n"," [0.09771712 0.12264585 0.09769337 0.09772117 0.09570528 0.09772755\n","  0.09769415 0.09770663 0.09772415 0.09766475]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] 4.444936206114044\n","[[0.09772035 0.12264565 0.09769603 0.09772378 0.09568377 0.09772992\n","  0.0976966  0.09770927 0.09772734 0.09766729]\n"," [0.09772035 0.12264565 0.09769603 0.09772378 0.09568377 0.09772992\n","  0.0976966  0.09770927 0.09772734 0.09766729]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.67235202774606\n","Epoch 53: train_loss = 2.262 | train_acc = 0.300 \n","[5 6 9 3 4 2 8 7 0 1]\n","[[0.09771873 0.12264745 0.09769444 0.09772228 0.09569382 0.09772856\n","  0.09769523 0.09770782 0.09772563 0.09766603]\n"," [0.09771873 0.12264745 0.09769444 0.09772228 0.09569382 0.09772856\n","  0.09769523 0.09770782 0.09772563 0.09766603]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 4.424351876650217\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-8af2c99c0478>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# backwards - to calculate the gradients w.r.t. cross entropy loss and auxiliary loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# update the weights by gradients obtained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/graduation_project/FPTT-on-ANN/RNN/lstm-master/new/lstm.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, t, y_label, trunc_h, trunc_s)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_wi_diff\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_wf_diff\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_wg_diff\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdg_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;31m# (128) = (200,128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# mean_train_loss_list = list()\n","# train_acc_list = list()\n","\n","\n","# # Initialize the batch size, the number of epochs, and the learning rate\n","# n_samples = 1 #X.shape[0]\n","\n","# batch_size = 1\n","# epochs = 10 #200\n","# lr = 0.01\n","# alpha = 0.1\n","\n","# # Initialize the weights\n","# # weights = init_weights(layers)\n","# # w_g, w_lbd, w_rm = init_weights_derivatives(layers, weights)\n","\n","# # Q: predicted label distribution of last training epoch --> used to make auxiliary loss (oracle loss)\n","# Q = np.zeros_like(Y)\n","\n","# target = np.zeros((batch_size, 10))\n","\n","\n","# # Epoch for loop\n","# for epoch in range(epochs):\n","\n","#     if (epoch==50) or (epoch==90) or (epoch==120):\n","#         lr *= 0.1\n","\n","#     # Initialize the layers\n","#     # ins, h, o = init_layer(layers,batch_size)\n","#     # initiate hidden states for current batch\n","#     Model = model.Lstm(seq_len, batch_size, layers, param)\n","   \n","#     # Initialize the training loss and accuracy for each epoch\n","#     train_loss = 0\n","#     train_acc = 0\n","\n","#     # Create a random permutation for shuffling\n","#     shuffle = np.random.permutation(n_samples)\n","#     print(shuffle)\n","\n","\n","#     # Shuffle dataset and create mini-batches for each epoch\n","#     X_batches = np.array_split(X[shuffle],n_samples/batch_size)\n","#     Y_batches = np.array_split(Y[shuffle],n_samples/batch_size)\n","#     Q_batches = np.array_split(Q[shuffle],n_samples/batch_size)\n","#     target = np.empty_like(Q_batches)\n","\n","#     print(Y_batches)\n","\n","#     # Mini-batch for loop\n","#     for b in range(int(n_samples/batch_size)):\n","\n","#         for t in range (784):          \n","#             Model.state.forward(t, X_batches[b][:, t:(t+1)] )\n","\n","#         # to make target for cross entropy and divergence term \n","#         # beta = (cnt+1)/seq_len \n","#         # target[b] = beta*(Y_batches[b]) + (1-beta)*Q_batches[b]\n","\n","#         # backwards - to calculate the gradients w.r.t. cross entropy loss and auxiliary loss\n","#         Model.backward(t, target[b])\n","\n","#         # update the weights by gradients obtained\n","#         # Model.param.apply_diff_fptt(lr, alpha)\n","#         Model.param.apply_diff(lr)\n","\n","#         # Q[shuffle[batch_size*b:batch_size*(b+1)]] = Q_batches[b]\n","\n","#         train_loss += cross_entropy(Model.state.Y_hat[seq_len-1],Y_batches[b])\n","#         train_acc += accuracy(labeling(Model.state.Y_hat[seq_len-1]),Y_batches[b])       \n","\n","\n","#     mean_train_loss = train_loss/n_samples\n","#     mean_train_loss_list.append(mean_train_loss)\n","#     train_acc = (train_acc/len(X_batches))\n","#     train_acc_list.append(train_acc)\n","\n","\n","\n","\n","    # print(f\"Epoch {epoch+1}: train_loss = {mean_train_loss:.3f} | train_acc = {train_acc:.3f} \" )\n","\n"],"metadata":{"id":"NdP-tAecpyg1","executionInfo":{"status":"aborted","timestamp":1686179660854,"user_tz":-120,"elapsed":14,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mean_train_loss_list = list()\n","# train_acc_list = list()\n","\n","\n","# # Initialize the batch size, the number of epochs, and the learning rate\n","# n_samples = 2 #X.shape[0]\n","\n","# batch_size = 1\n","# epochs = 200\n","# lr = 0.01\n","# alpha = 0.1\n","\n","# # Initialize the weights\n","# # weights = init_weights(layers)\n","# # w_g, w_lbd, w_rm = init_weights_derivatives(layers, weights)\n","\n","# # Q: predicted label distribution of last training epoch --> used to make auxiliary loss (oracle loss)\n","# Q = np.zeros_like(Y)\n","\n","# target = np.zeros((batch_size, 10))\n","\n","\n","# # Epoch for loop\n","# for epoch in range(epochs):\n","\n","#     # if (epoch==60) or (epoch==90) or (epoch==120):\n","#         # lr *= 0.5\n","\n","#     # Initialize the layers\n","#     # ins, h, o = init_layer(layers,batch_size)\n","#     # initiate hidden states for current batch\n","#     Model = model.Lstm(seq_len, batch_size, layers, param)\n","   \n","#     # Initialize the training loss and accuracy for each epoch\n","#     train_loss = 0\n","#     train_acc = 0\n","\n","#     # Create a random permutation for shuffling\n","#     shuffle = np.random.permutation(n_samples)\n","#     # print(shuffle[0])\n","\n","\n","#     # Shuffle dataset and create mini-batches for each epoch\n","#     X_batches = np.array_split(X[shuffle],n_samples/batch_size)\n","#     Y_batches = np.array_split(Y[shuffle],n_samples/batch_size)\n","#     Q_batches = np.array_split(Q[shuffle],n_samples/batch_size)\n","#     target = np.empty_like(Q_batches)\n","\n"," \n","\n","#     # Mini-batch for loop\n","#     for b in range(int(n_samples/batch_size)):\n","\n","#         for t in range(seq_len):\n","            \n","#             Model.state.forward(t, X_batches[b][:,t:t+1])\n","\n","#             # to make target for cross entropy and divergence term \n","#             beta = (t+1)/seq_len \n","#             target[b] = beta*(Y_batches[b]) + (1-beta)*Q_batches[b]\n","\n","#             # backwards - to calculate the gradients w.r.t. cross entropy loss and auxiliary loss\n","#             Model.backward(t, target[b], trunc_h=0, trunc_s=0)\n","\n","#             # update the weights by gradients obtained\n","#             Model.param.apply_diff_fptt(lr, alpha)\n","\n","\n","#         Q[shuffle[batch_size*b:batch_size*(b+1)]] = Q_batches[b]\n","\n","#         train_loss += cross_entropy(Model.state.Y_hat[seq_len-1],Y_batches[b])\n","#         train_acc += accuracy(labeling(Model.state.Y_hat[seq_len-1]),Y_batches[b])       \n","\n","\n","#     mean_train_loss = train_loss/n_samples\n","#     mean_train_loss_list.append(mean_train_loss)\n","#     train_acc = (train_acc/len(X_batches))\n","#     train_acc_list.append(train_acc)\n","\n","\n","\n","\n","#     print(f\"Epoch {epoch+1}: train_loss = {mean_train_loss:.3f} | train_acc = {train_acc:.3f} \" )\n","\n"],"metadata":{"executionInfo":{"status":"aborted","timestamp":1686179660854,"user_tz":-120,"elapsed":13,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}},"id":"6TVautln-NDB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(1,2,figsize=(15,5))\n","\n","ax[0].plot(mean_train_loss_list,label=\"Train loss\")\n","# ax[0].plot(mean_test_loss_list,label=\"Test loss\")\n","ax[0].legend()\n","ax[0].set_xlabel(\"Epoch\")\n","ax[0].set_ylabel(\"Loss\")\n","ax[0].grid()\n","\n","ax[1].plot(train_acc_list,label=\"Train accuracy\")\n","# ax[1].plot(test_acc_list,label=\"Test accuracy\")\n","ax[1].legend()\n","ax[1].set_xlabel(\"Epoch\")\n","ax[1].set_ylabel(\"Accuracy\")\n","ax[1].grid()"],"metadata":{"id":"FmRnCQqf3RiZ","executionInfo":{"status":"aborted","timestamp":1686179660855,"user_tz":-120,"elapsed":14,"user":{"displayName":"Yicheng Zhang","userId":"07157030176331289981"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"weQHXPA8Kzkj"},"execution_count":null,"outputs":[]}]}